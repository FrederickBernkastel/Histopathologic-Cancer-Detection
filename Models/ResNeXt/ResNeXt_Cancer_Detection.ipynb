{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNeXt_Cancer_Detection.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFO16PF0xgwt",
        "colab_type": "code",
        "outputId": "75407b82-99ba-46a8-8e3d-ef9c6d4ed48b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.2)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbcgxCx8xstA",
        "colab_type": "code",
        "outputId": "b7ccd03b-699f-414e-8eaa-fbe3c094efbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx8Hex00xyX-",
        "colab_type": "code",
        "outputId": "bb309bfb-4a08-4aec-d07d-d96f4263e790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrL4C5-xx613",
        "colab_type": "code",
        "outputId": "f961f5bb-a0fe-432a-aa57-0206611964d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "!kaggle competitions download -c histopathologic-cancer-detection -p /content/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/1.33M [00:00<?, ?B/s]\n",
            "100% 1.33M/1.33M [00:00<00:00, 42.9MB/s]\n",
            "Downloading train_labels.csv.zip to /content\n",
            "  0% 0.00/5.10M [00:00<?, ?B/s]\n",
            "100% 5.10M/5.10M [00:00<00:00, 45.9MB/s]\n",
            "Downloading test.zip to /content\n",
            " 99% 1.29G/1.30G [00:11<00:00, 143MB/s]\n",
            "100% 1.30G/1.30G [00:11<00:00, 122MB/s]\n",
            "Downloading train.zip to /content\n",
            "100% 4.98G/4.98G [02:49<00:00, 18.6MB/s]\n",
            "100% 4.98G/4.98G [02:49<00:00, 31.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjKEU79hyBBg",
        "colab_type": "code",
        "outputId": "04fce6d4-d3e1-4a59-ec45-d2f906d83e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "%cd /content\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "adc.json     sample_submission.csv.zip\ttest.zip  train_labels.csv.zip\n",
            "sample_data  test\t\t\ttrain\t  train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpCWs-XXx9e0",
        "colab_type": "code",
        "outputId": "82e1bd62-9210-4417-f11c-2d272bd49d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!mkdir /content/test\n",
        "!unzip -qq /content/test.zip -d /content/test\n",
        "!rm /content/test.zip\n",
        "!mkdir /content/train\n",
        "!unzip -qq /content/train.zip -d /content/train\n",
        "!rm /content/train.zip\n",
        "!unzip -qq /content/train_labels.csv.zip\n",
        "!rm /content/train_labels.csv.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/test’: File exists\n",
            "mkdir: cannot create directory ‘/content/train’: File exists\n",
            "adc.json  sample_data  sample_submission.csv.zip  test\ttrain  train_labels.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuEv4_oayFoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gc\n",
        "import os\n",
        "from glob import glob\n",
        "from random import shuffle\n",
        "import cv2\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from albumentations import Compose, RandomRotate90, Transpose, Flip, OneOf, CLAHE, IAASharpen, IAAEmboss, RandomBrightnessContrast, JpegCompression, Blur, GaussNoise, HueSaturationValue, ShiftScaleRotate, Normalize\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.losses import binary_crossentropy\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
        "from tensorflow.python.keras.layers import Lambda, Reshape, DepthwiseConv2D, ZeroPadding2D, Add, MaxPooling2D,Activation, Flatten, Conv2D, Dense, Input, Dropout, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkuQlNgHywDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_applications import get_submodules_from_kwargs\n",
        "from keras.applications import keras_modules_injection\n",
        "\n",
        "backend = None\n",
        "keras_utils = None\n",
        "\n",
        "\n",
        "BASE_WEIGHTS_PATH = (\n",
        "    'https://github.com/keras-team/keras-applications/'\n",
        "    'releases/download/resnet/')\n",
        "WEIGHTS_HASHES = {\n",
        "    'resnet50': ('2cb95161c43110f7111970584f804107',\n",
        "                 '4d473c1dd8becc155b73f8504c6f6626'),\n",
        "    'resnext50': ('67a5b30d522ed92f75a1f16eef299d1a',\n",
        "                  '62527c363bdd9ec598bed41947b379fc')\n",
        "}\n",
        "\n",
        "def block1(x, filters, kernel_size=3, stride=1,\n",
        "           conv_shortcut=True, name=None):\n",
        "    \"\"\"A residual block.\n",
        "\n",
        "    # Arguments\n",
        "        x: input tensor.\n",
        "        filters: integer, filters of the bottleneck layer.\n",
        "        kernel_size: default 3, kernel size of the bottleneck layer.\n",
        "        stride: default 1, stride of the first layer.\n",
        "        conv_shortcut: default True, use convolution shortcut if True,\n",
        "            otherwise identity shortcut.\n",
        "        name: string, block label.\n",
        "\n",
        "    # Returns\n",
        "        Output tensor for the residual block.\n",
        "    \"\"\"\n",
        "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    if conv_shortcut is True:\n",
        "        shortcut = Conv2D(4 * filters, 1, strides=stride,\n",
        "                                 name=name + '_0_conv')(x)\n",
        "        shortcut = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                             name=name + '_0_bn')(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "\n",
        "    x = Conv2D(filters, 1, strides=stride, name=name + '_1_conv')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                 name=name + '_1_bn')(x)\n",
        "    x = Activation('relu', name=name + '_1_relu')(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='SAME',\n",
        "                    name=name + '_2_conv')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                 name=name + '_2_bn')(x)\n",
        "    x = Activation('relu', name=name + '_2_relu')(x)\n",
        "\n",
        "    x = Conv2D(4 * filters, 1, name=name + '_3_conv')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                  name=name + '_3_bn')(x)\n",
        "\n",
        "    x = Add(name=name + '_add')([shortcut, x])\n",
        "    x = Activation('relu', name=name + '_out')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def stack1(x, filters, blocks, stride1=2, name=None):\n",
        "    \"\"\"A set of stacked residual blocks.\n",
        "\n",
        "    # Arguments\n",
        "        x: input tensor.\n",
        "        filters: integer, filters of the bottleneck layer in a block.\n",
        "        blocks: integer, blocks in the stacked blocks.\n",
        "        stride1: default 2, stride of the first layer in the first block.\n",
        "        name: string, stack label.\n",
        "\n",
        "    # Returns\n",
        "        Output tensor for the stacked blocks.\n",
        "    \"\"\"\n",
        "    x = block1(x, filters, stride=stride1, name=name + '_block1')\n",
        "    for i in range(2, blocks + 1):\n",
        "        x = block1(x, filters, conv_shortcut=False, name=name + '_block' + str(i))\n",
        "    return x\n",
        "\n",
        "def block3(x, filters, kernel_size=3, stride=1, groups=32,\n",
        "           conv_shortcut=True, name=None):\n",
        "    \"\"\"A residual block.\n",
        "\n",
        "    # Arguments\n",
        "        x: input tensor.\n",
        "        filters: integer, filters of the bottleneck layer.\n",
        "        kernel_size: default 3, kernel size of the bottleneck layer.\n",
        "        stride: default 1, stride of the first layer.\n",
        "        groups: default 32, group size for grouped convolution.\n",
        "        conv_shortcut: default True, use convolution shortcut if True,\n",
        "            otherwise identity shortcut.\n",
        "        name: string, block label.\n",
        "\n",
        "    # Returns\n",
        "        Output tensor for the residual block.\n",
        "    \"\"\"\n",
        "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
        "\n",
        "    if conv_shortcut is True:\n",
        "        shortcut = Conv2D((64 // groups) * filters, 1, strides=stride,\n",
        "                                 use_bias=False, name=name + '_0_conv')(x)\n",
        "        shortcut = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                             name=name + '_0_bn')(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "\n",
        "    x = Conv2D(filters, 1, use_bias=False, name=name + '_1_conv')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                  name=name + '_1_bn')(x)\n",
        "    x = Activation('relu', name=name + '_1_relu')(x)\n",
        "\n",
        "    c = filters // groups\n",
        "    x = ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\n",
        "    x = DepthwiseConv2D(kernel_size, strides=stride, depth_multiplier=c,\n",
        "                               use_bias=False, name=name + '_2_conv')(x)\n",
        "    x_shape = backend.int_shape(x)[1:-1]\n",
        "    x = Reshape(x_shape + (groups, c, c))(x)\n",
        "    output_shape = x_shape + (groups, c) if backend.backend() == 'theano' else None\n",
        "    x = Lambda(lambda x: sum([x[:, :, :, :, i] for i in range(c)]),\n",
        "                      output_shape=output_shape, name=name + '_2_reduce')(x)\n",
        "    x = Reshape(x_shape + (filters,))(x)\n",
        "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                  name=name + '_2_bn')(x)\n",
        "    x = Activation('relu', name=name + '_2_relu')(x)\n",
        "\n",
        "    x = Conv2D((64 // groups) * filters, 1,\n",
        "                      use_bias=False, name=name + '_3_conv')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                  name=name + '_3_bn')(x)\n",
        "\n",
        "    x = Add(name=name + '_add')([shortcut, x])\n",
        "    x = Activation('relu', name=name + '_out')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def stack3(x, filters, blocks, stride1=2, groups=32, name=None):\n",
        "    \"\"\"A set of stacked residual blocks.\n",
        "\n",
        "    # Arguments\n",
        "        x: input tensor.\n",
        "        filters: integer, filters of the bottleneck layer in a block.\n",
        "        blocks: integer, blocks in the stacked blocks.\n",
        "        stride1: default 2, stride of the first layer in the first block.\n",
        "        groups: default 32, group size for grouped convolution.\n",
        "        name: string, stack label.\n",
        "\n",
        "    # Returns\n",
        "        Output tensor for the stacked blocks.\n",
        "    \"\"\"\n",
        "    x = block3(x, filters, stride=stride1, groups=groups, name=name + '_block1')\n",
        "    for i in range(2, blocks + 1):\n",
        "        x = block3(x, filters, groups=groups, conv_shortcut=False,\n",
        "                   name=name + '_block' + str(i))\n",
        "    return x\n",
        "\n",
        "def ResNet(stack_fn,\n",
        "           preact,\n",
        "           use_bias,\n",
        "           model_name='resnet',\n",
        "           weights='imagenet',\n",
        "           input_tensor=None,\n",
        "           **kwargs):\n",
        "    \"\"\"Instantiates the ResNet, ResNeXt architecture.\n",
        "\n",
        "    Optionally loads weights pre-trained on ImageNet.\n",
        "    Note that the data format convention used by the model is\n",
        "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
        "\n",
        "    # Arguments\n",
        "        stack_fn: a function that returns output tensor for the\n",
        "            stacked residual blocks.\n",
        "        preact: whether to use pre-activation or not\n",
        "            (True for ResNetV2, False for ResNet and ResNeXt).\n",
        "        use_bias: whether to use biases for convolutional layers or not\n",
        "            (True for ResNet and ResNetV2, False for ResNeXt).\n",
        "        model_name: string, model name.\n",
        "        weights: one of `None` (random initialization),\n",
        "              'imagenet' (pre-training on ImageNet),\n",
        "              or the path to the weights file to be loaded.\n",
        "        input_tensor: tensor\n",
        "            (i.e. output of `layers.Input()`)\n",
        "            to use as image input for the model.\n",
        "\n",
        "\n",
        "    # Returns\n",
        "        A Keras model instance.the output of the model will be\n",
        "                the 4D tensor output of the\n",
        "                last convolutional layer\n",
        "\n",
        "    # Raises\n",
        "        ValueError: in case of invalid argument for `weights`,\n",
        "            or invalid input shape.\n",
        "    \"\"\"\n",
        "    global backend, keras_utils\n",
        "    backend, _, _, keras_utils = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization), `imagenet` '\n",
        "                         '(pre-training on ImageNet), '\n",
        "                         'or the path to the weights file to be loaded.')\n",
        "\n",
        "    if input_tensor is None:\n",
        "       raise ValueError(\"input_tensor can't be None\")\n",
        "    \n",
        "    img_input = input_tensor\n",
        "    \n",
        "    x = ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(img_input)\n",
        "    x = Conv2D(64, 7, strides=2, use_bias=use_bias, name='conv1_conv')(x)\n",
        "\n",
        "    if preact is False:\n",
        "        x = BatchNormalization(axis=3, epsilon=1.001e-5,\n",
        "                                      name='conv1_bn')(x)\n",
        "        x = Activation('relu', name='conv1_relu')(x)\n",
        "\n",
        "    x = ZeroPadding2D(padding=((1, 1), (1, 1)), name='pool1_pad')(x)\n",
        "    x = MaxPooling2D(3, strides=2, name='pool1_pool')(x)\n",
        "\n",
        "    x = stack_fn(x)\n",
        "\n",
        "    if preact is True:\n",
        "        x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n",
        "                                      name='post_bn')(x)\n",
        "        x = Activation('relu', name='post_relu')(x)\n",
        "\n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, x, name=model_name)\n",
        "\n",
        "    # Load weights.\n",
        "    if (weights == 'imagenet') and (model_name in WEIGHTS_HASHES):        \n",
        "        file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "        file_hash = WEIGHTS_HASHES[model_name][1]\n",
        "        weights_path = keras_utils.get_file(file_name,\n",
        "                                            BASE_WEIGHTS_PATH + file_name,\n",
        "                                            cache_subdir='models',\n",
        "                                            file_hash=file_hash)\n",
        "        model.load_weights(weights_path)\n",
        "    elif weights is not None:\n",
        "        model.load_weights(weights)\n",
        "\n",
        "    return model\n",
        "\n",
        "@keras_modules_injection\n",
        "def ResNet50(weights='imagenet', input_tensor=None, **kwargs):\n",
        "  def stack_fn(x):\n",
        "      x = stack1(x, 64, 3, stride1=1, name='conv2')\n",
        "      x = stack1(x, 128, 4, name='conv3')\n",
        "      x = stack1(x, 256, 6, name='conv4')\n",
        "      x = stack1(x, 512, 3, name='conv5')\n",
        "      return x\n",
        "  return ResNet(stack_fn, False, True, 'resnet50',\n",
        "                weights, input_tensor, **kwargs)\n",
        "\n",
        "@keras_modules_injection\n",
        "def ResNeXt50(weights='imagenet', input_tensor=None, **kwargs):\n",
        "    def stack_fn(x):\n",
        "        x = stack3(x, 128, 3, stride1=1, name='conv2')\n",
        "        x = stack3(x, 256, 4, name='conv3')\n",
        "        x = stack3(x, 512, 6, name='conv4')\n",
        "        x = stack3(x, 1024, 3, name='conv5')\n",
        "        return x\n",
        "    return ResNet(stack_fn, False, False, 'resnext50',\n",
        "                  weights, input_tensor, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6et1qH04y4hK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_id_from_file_path(file_path):\n",
        "    return file_path.split(os.path.sep)[-1].replace('.tif', '')\n",
        "\n",
        "\n",
        "def chunker(seq, size):\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
        "\n",
        "\n",
        "def do_train_augmentations():\n",
        "    return Compose([\n",
        "        RandomRotate90(p=0.5),\n",
        "        Transpose(p=0.5),\n",
        "        Flip(p=0.5),\n",
        "        OneOf([CLAHE(clip_limit=2),\n",
        "              IAASharpen(),\n",
        "              IAAEmboss(),\n",
        "              RandomBrightnessContrast(),\n",
        "              JpegCompression(),\n",
        "              Blur(),\n",
        "              GaussNoise()],\n",
        "              p=0.5),\n",
        "        HueSaturationValue(p=0.5),\n",
        "        ShiftScaleRotate(shift_limit=0.15, scale_limit=0.15, rotate_limit=45, p=0.5),\n",
        "        Normalize(p=1)])\n",
        "\n",
        "\n",
        "def do_inference_aug():\n",
        "    return Compose([Normalize(p=1)], p=1)\n",
        "\n",
        "\n",
        "def data_gen(list_files, id_label_map_in, batch_size_in, img_size_in, aug_funtion):\n",
        "  \n",
        "    aug = aug_funtion()\n",
        "    \n",
        "    while True:\n",
        "      \n",
        "        shuffle(list_files)\n",
        "        for block in chunker(list_files, batch_size_in):\n",
        "\n",
        "            X = [cv2.resize(cv2.imread(x), (img_size_in, img_size_in)) for x in block]\n",
        "            X = [aug(image=x)['image'] for x in X]\n",
        "\n",
        "            Y = [id_label_map_in[get_id_from_file_path(x)] for x in block]\n",
        "\n",
        "            yield np.array(X), np.array(Y)\n",
        "\n",
        "\n",
        "def get_model(img_size, batch_size):\n",
        "  \n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    visible = Input(shape=(img_size, img_size, 3), batch_size=batch_size, dtype=tf.float32)\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(visible)\n",
        "    conv1_act = Activation('elu')(conv1)\n",
        "    conv1_act_batch = BatchNormalization()(conv1_act)\n",
        "\n",
        "    conv2 = Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(conv1_act_batch)\n",
        "    conv2_act = Activation('elu')(conv2)\n",
        "    conv2_act_batch = BatchNormalization()(conv2_act)\n",
        "    conv2_act_batch_max = MaxPooling2D(pool_size=(2, 2))(conv2_act_batch)\n",
        "    conv2_act_batch_max_drop = Dropout(0.2)(conv2_act_batch_max)\n",
        "\n",
        "    conv3 = Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(\n",
        "        conv2_act_batch_max_drop)\n",
        "    conv3_act = Activation('elu')(conv3)\n",
        "    conv3_act_batch = BatchNormalization()(conv3_act)\n",
        "\n",
        "    conv4 = Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(conv3_act_batch)\n",
        "    conv4_act = Activation('elu')(conv4)\n",
        "    conv4_act_batch = BatchNormalization()(conv4_act)\n",
        "    conv4_act_batch_max = MaxPooling2D(pool_size=(2, 2))(conv4_act_batch)\n",
        "    conv4_act_batch_max_drop = Dropout(0.3)(conv4_act_batch_max)\n",
        "\n",
        "    conv5 = Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(\n",
        "        conv4_act_batch_max_drop)\n",
        "    conv5_act = Activation('elu')(conv5)\n",
        "    conv5_act_batch = BatchNormalization()(conv5_act)\n",
        "\n",
        "    conv6 = Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay))(conv5_act_batch)\n",
        "    conv6_act = Activation('elu')(conv6)\n",
        "    conv6_act_batch = BatchNormalization()(conv6_act)\n",
        "    conv6_act_batch_max = MaxPooling2D(pool_size=(2, 2))(conv6_act_batch)\n",
        "    conv6_act_batch_max_drop = Dropout(0.4)(conv6_act_batch_max)\n",
        "    \n",
        "    flat = Flatten()(conv6_act_batch_max_drop)\n",
        "\n",
        "    # and a logistic layer\n",
        "    predictions = Dense(1, activation='sigmoid')(flat)\n",
        "    \n",
        "    # Create model.\n",
        "    model = tf.keras.Model(visible, predictions, name='baseline')\n",
        "\n",
        "    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001,\n",
        "    beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam'),\n",
        "    loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_model_resnet50(img_size, batch_size):\n",
        "  \n",
        "    inputs = Input(shape=(img_size, img_size, 3), batch_size=batch_size, dtype=tf.float32)\n",
        "  \n",
        "    base_model = ResNet50(input_tensor = inputs)\n",
        "    x = base_model(inputs)\n",
        "    out1 = GlobalMaxPooling2D()(x)\n",
        "    out2 = GlobalAveragePooling2D()(x)\n",
        "    out3 = Flatten()(x)\n",
        "    out = Concatenate(axis=-1)([out1, out2, out3])\n",
        "    out = Dropout(0.5)(out)\n",
        "    \n",
        "    # and a logistic layer\n",
        "    out = Dense(1, activation=\"sigmoid\", name=\"3_\")(out)\n",
        "    \n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, out)\n",
        "    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.00007,\n",
        "    beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam'),\n",
        "    loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    return model\n",
        "  \n",
        "def get_model_resnext50(img_size, batch_size):\n",
        "  \n",
        "    inputs = Input(shape=(img_size, img_size, 3), batch_size=batch_size, dtype=tf.float32)\n",
        "  \n",
        "    base_model = ResNeXt50(input_tensor = inputs)\n",
        "    x = base_model(inputs)\n",
        "    out1 = GlobalMaxPooling2D()(x)\n",
        "    out2 = GlobalAveragePooling2D()(x)\n",
        "    out3 = Flatten()(x)\n",
        "    out = Concatenate(axis=-1)([out1, out2, out3])\n",
        "    out = Dropout(0.5)(out)\n",
        "    \n",
        "    # and a logistic layer\n",
        "    out = Dense(1, activation=\"sigmoid\", name=\"3_\")(out)\n",
        "    \n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, out)\n",
        "    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.00007,\n",
        "    beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam'),\n",
        "    loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def plot_metric(history_in, metric_name, results_dir):\n",
        "    \"\"\"\n",
        "    Plot a metric of model's history.\n",
        "    \"\"\"\n",
        "\n",
        "    fig_acc = plt.figure(figsize=(10, 10))\n",
        "    plt.plot(history_in.history[metric_name])\n",
        "    plt.plot(history_in.history['val_' + metric_name])\n",
        "\n",
        "    plt.title('model ' + metric_name)\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    fig_acc.savefig(results_dir+\"/model_\" + metric_name + \".png\")\n",
        "\n",
        "    plt.cla()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def mkdir_if_not_exist(directory):\n",
        "\n",
        "        if not os.path.exists(directory):\n",
        "            try:\n",
        "                os.makedirs(directory)\n",
        "            except OSError as e:\n",
        "                print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvfu7EI20xu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Configuration\n",
        "#@markdown \n",
        "\n",
        "output_path = 'output'  #@param\n",
        "train_limit = 220025  #@param {type: \"number\"}\n",
        "test_limit = 57458  #@param {type: \"number\"}\n",
        "epochs = 10 #@param {type:\"integer\"}\n",
        "n_fold = 4  #@param {type: \"slider\", min: 2, max: 10}\n",
        "number_of_tpu_core = 8  #@param {type: \"slider\", min: 1, max: 8}\n",
        "batch_size = 88  #@param {type: \"slider\", min: 8, max: 128}\n",
        "#@markdown ---\n",
        "architecture = \"ResNext50\" #@param [\"Baseline\", \"Resnet50\", \"ResNext50\"]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFn3l9XJ039j",
        "colab_type": "code",
        "outputId": "13c78cb7-2525-4418-dfe6-cea4786dee5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50570
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  \n",
        "    ###############\n",
        "    # Configuration\n",
        "    ###############\n",
        "    img_size = 96\n",
        "    training_batch_size = batch_size * number_of_tpu_core\n",
        "    \n",
        "    \n",
        "    ##################\n",
        "    # Data Preparation\n",
        "    ##################\n",
        "    \n",
        "    df_train = pd.read_csv(\"/content/train_labels.csv\")\n",
        "    id_label_map = {k: v for k, v in zip(df_train.id.values, df_train.label.values)}\n",
        "\n",
        "    labeled_files = glob('/content/train/*.tif')\n",
        "    test_files = glob('/content/test/*.tif')\n",
        "\n",
        "    print(\"labeled_files size :\", len(labeled_files))\n",
        "    print(\"test_files size :\", len(test_files))\n",
        "\n",
        "    df_dataset = pd.DataFrame()\n",
        "    df_dataset['id'] = labeled_files[0:train_limit]\n",
        "    df_dataset['label'] = df_train['label'].iloc[0:train_limit]\n",
        "\n",
        "    ensemble_preds = np.zeros(len(test_files[0:test_limit]), dtype=np.float)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_fold)\n",
        "\n",
        "    for fold in range(0, n_fold):\n",
        "\n",
        "        print(\"\\nFOLD: {}\".format(fold))\n",
        "\n",
        "        mkdir_if_not_exist(os.path.join(output_path, str(fold)))\n",
        "\n",
        "        h5_path = output_path + \"/model_\"+str(fold)+\".h5\"\n",
        "\n",
        "        callbacks = [\n",
        "            TerminateOnNaN(),\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=5,\n",
        "                mode='min',\n",
        "                verbose=1,\n",
        "                restore_best_weights=True),\n",
        "            ModelCheckpoint(\n",
        "                h5_path,\n",
        "                monitor='val_loss',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                mode='min')]\n",
        "\n",
        "        result = next(skf.split(X=df_dataset, y=df_dataset['label']), None)\n",
        "\n",
        "        train = df_dataset.iloc[result[0]]['id'].tolist()\n",
        "        val = df_dataset.iloc[result[1]]['id'].tolist()\n",
        "\n",
        "        print(\"train:\")\n",
        "        unique, counts = np.unique(df_dataset.iloc[result[0]]['label'].values, return_counts=True)\n",
        "        print(dict(zip(unique, counts)))\n",
        "\n",
        "        print(\"val:\")\n",
        "        unique, counts = np.unique(df_dataset.iloc[result[1]]['label'].values, return_counts=True)\n",
        "        print(dict(zip(unique, counts)))\n",
        "        \n",
        "        ######################\n",
        "        # Put the model on TPU\n",
        "        ######################\n",
        "        \n",
        "        tf.keras.backend.clear_session()\n",
        "        if architecture == \"Baseline\":\n",
        "          training_model = get_model(img_size, batch_size)\n",
        "        elif architecture == \"Resnet50\":\n",
        "          training_model = get_model_resnet50(img_size, batch_size)\n",
        "        elif architecture == \"ResNext50\":\n",
        "           training_model = get_model_resnext50(img_size, batch_size)\n",
        "               \n",
        "        # This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "        TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "        tf.logging.set_verbosity(tf.logging.INFO)\n",
        "        \n",
        "        # Converting the Keras model to TPU model using tf.contrib.tpu.keras_to_tpu_model\n",
        "        model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "            training_model,\n",
        "            strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        ###########\n",
        "        # Training\n",
        "        ###########\n",
        "\n",
        "        history = model.fit_generator(\n",
        "            data_gen(train, id_label_map, training_batch_size, img_size, do_train_augmentations),\n",
        "            validation_data=data_gen(val, id_label_map, training_batch_size, img_size, do_inference_aug),\n",
        "            epochs=epochs, verbose=1,\n",
        "            callbacks=callbacks,\n",
        "            steps_per_epoch=len(train) // training_batch_size,\n",
        "            validation_steps=len(val) // training_batch_size)\n",
        "\n",
        "        # summarize history\n",
        "#         plot_metric(history, 'loss', str(fold))\n",
        "#         plot_metric(history, 'acc', str(fold))\n",
        "\n",
        "        #################\n",
        "        # Kaggle testing\n",
        "        #################\n",
        "        model.load_weights(h5_path)\n",
        "\n",
        "        preds = []\n",
        "        ids = []\n",
        "\n",
        "        predict_batch_size = 128 * 8\n",
        "        counter = 0\n",
        "        aug = do_inference_aug()\n",
        "        \n",
        "        for batch in chunker(test_files[0:test_limit], predict_batch_size):\n",
        "          \n",
        "            print(\"Indexes: %i - %i\" % (counter, counter + predict_batch_size))\n",
        "            \n",
        "            counter += predict_batch_size\n",
        "            X = [aug(image=(cv2.resize(cv2.imread(x), (img_size, img_size))))['image'] for x in batch]\n",
        "            ids_batch = [get_id_from_file_path(x) for x in batch]\n",
        "            \n",
        "            # predict_batch_size must be divisible by the number of TPU cores in use\n",
        "            dummy_rows = len(batch) % number_of_tpu_core\n",
        "            if dummy_rows > 0:\n",
        "              for i in range(0, number_of_tpu_core - dummy_rows):\n",
        "                X.append(np.zeros((img_size,img_size,3), dtype=np.float32))\n",
        "            \n",
        "            X = np.array(X)\n",
        "            \n",
        "            preds_batch = ((model.predict(X).ravel()*model.predict(X[:, ::-1, :, :]).ravel()*model.predict(X[:, ::-1, ::-1, :]).\n",
        "                            ravel()*model.predict(X[:, :, ::-1, :]).ravel())**0.25).tolist()\n",
        "            \n",
        "            preds += preds_batch\n",
        "            ids += ids_batch\n",
        "\n",
        "        df = pd.DataFrame({'id': ids, 'label': preds[0:-(number_of_tpu_core - dummy_rows)]})\n",
        "        df.to_csv(\"results_\"+str(fold)+\".csv\", index=False)\n",
        "        print(df.head())\n",
        "\n",
        "        # sum the predicted values\n",
        "        ensemble_preds += np.array(preds[0:-(number_of_tpu_core - dummy_rows)], dtype=np.float)\n",
        "        \n",
        "        # in order to release the memory\n",
        "        del history\n",
        "        del model\n",
        "        gc.collect()\n",
        "        tf.keras.backend.clear_session()\n",
        "# average the predicted values\n",
        "ensemble_preds /= n_fold\n",
        "\n",
        "df = pd.DataFrame({'id': ids, 'label': ensemble_preds.ravel()})\n",
        "df.to_csv(\"ensemble.csv\", index=False)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeled_files size : 220025\n",
            "test_files size : 57458\n",
            "\n",
            "FOLD: 0\n",
            "train:\n",
            "{0: 98181, 1: 66837}\n",
            "val:\n",
            "{0: 32727, 1: 22280}\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.69.248.170:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 16420511637204072341)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6510740910710379258)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 340279278997891419)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4519146210501702623)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5625607179061829023)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8036702111319775671)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7869912675267070857)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14387975401566608843)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 1825262097596701992)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4647631621340637215)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 608507869667962858)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (128, 96, 96, 3)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "resnext50 (Model)               (128, 3, 3, 2048)    23048128    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (128, 2048)          0           resnext50[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (128, 2048)          0           resnext50[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (128, 18432)         0           resnext50[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (128, 22528)         0           global_max_pooling2d[0][0]       \n",
            "                                                                 global_average_pooling2d[0][0]   \n",
            "                                                                 flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (128, 22528)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "3_ (Dense)                      (128, 1)             22529       dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,070,657\n",
            "Trainable params: 23,002,433\n",
            "Non-trainable params: 68,224\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 96, 96, 3), dtype=tf.float32, name='input_1_10'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='3__target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Remapping placeholder for input_1\n",
            "INFO:tensorflow:Started compiling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e3c1a07e33b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mtraining_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             validation_steps=len(val) // training_batch_size)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# summarize history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0minput_specs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfeed_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_input_specs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m     tpu_model_ops = self._tpu_model_ops_for_input_specs(input_specs,\n\u001b[0;32m-> 1260\u001b[0;31m                                                         infeed_manager)\n\u001b[0m\u001b[1;32m   1261\u001b[0m     \u001b[0minfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfeed_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_tpu_model_ops_for_input_specs\u001b[0;34m(self, input_specs, infeed_manager)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                                                  infeed_manager)\n\u001b[1;32m   1170\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_model_compiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_test_model_compiles\u001b[0;34m(self, tpu_model_ops)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_error_message\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m       raise RuntimeError('Compilation failed: {}'.format(\n\u001b[0;32m-> 1114\u001b[0;31m           proto.status_error_message))\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Compilation failed: Compilation failure: Ran out of memory in memory space hbm. Used 9.99G of 8.00G hbm. Exceeded hbm capacity by 1.99G.\n\nTotal hbm usage >= 9.99G:\n    reserved        528.00M \n    program           9.47G \n    arguments       unknown size \n\nOutput size unknown.\n\nProgram hbm requirement 9.47G:\n    reserved          12.0K\n    global           192.0K\n    scoped            13.0K\n    HLO temp          9.47G (7.6% utilization, 0.4% fragmentation (37.92M))\n\n  Largest program allocations in hbm:\n\n  1. Size: 4.50G\n     Operator: op_type=\"Reshape\" op_name=\"tpu_139858372878176/resnext50/reshape_4/Reshape\"\n     Shape: f32[128,24,24,32,4,4]{5,3,0,2,1,4}\n     Unpadded size: 144.00M\n     Extra memory due to padding: 4.36G (32.0x expansion)\n     XLA label: %copy.1464 = f32[128,24,24,32,4,4]{5,3,0,2,1,4} copy(f32[128,24,24,32,4,4]{5,4,3,0,2,1} %reshape.22), metadata={op_type=\"Reshape\" op_name=\"tpu_139858372878176/resnext50/reshape_4/Reshape\"}\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 4.50G\n     Operator: op_type=\"Reshape\" op_name=\"tpu_139858372878176/resnext50/reshape_4/Reshape\"\n     Shape: f32[128,24,24,32,4,4]{5,4,3,0,2,1}\n     Unpadded size: 144.00M\n     Extra memory due to padding: 4.36G (32.0x expansion)\n     XLA label: %reshape.22 = f32[128,24,24,32,4,4]{5,4,3,0,2,1} reshape(f32[128,24,24,512]{3,0,2,1} %fusion.1040), metadata={op_type=\"Reshape\" op_name=\"tpu_139858372878176/resnext50/reshape_4/Reshape\"}\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 72.00M\n     Operator: op_type=\"Relu\" op_name=\"tpu_139858372878176/conv2_block1_out/Relu\"\n     Shape: f32[128,24,24,256]{3,0,2,1}\n     Unpadded size: 72.00M\n     XLA label: %fusion.94 = (f32[128,24,24,256]{3,0,2,1}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[256]{0} %get-tuple-element.4304, f32[256]{0} %get-tuple-element.4305, f32[256]{0} %get-tuple-element.3965, f32[128,24,24,256]{3,0,2,1} %get-tuple-element.3831, f32[256]{0} %...\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 72.00M\n     Operator: op_type=\"Relu\" op_name=\"tpu_139858372878176/conv2_block1_out/Relu\"\n     Shape: f32[128,24,24,256]{3,0,2,1}\n     Unpadded size: 72.00M\n     XLA label: %fusion.94 = (f32[128,24,24,256]{3,0,2,1}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[256]{0} %get-tuple-element.4304, f32[256]{0} %get-tuple-element.4305, f32[256]{0} %get-tuple-element.3965, f32[128,24,24,256]{3,0,2,1} %get-tuple-element.3831, f32[256]{0} %...\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 72.00M\n     Operator: op_type=\"Relu\" op_name=\"tpu_139858372878176/resnext50/conv2_block2_out/Relu\"\n     Shape: f32[128,24,24,256]{3,0,2,1}\n     Unpadded size: 72.00M\n     XLA label: %fusion.105 = f32[128,24,24,256]{3,0,2,1} fusion(f32[128,24,24,256]{3,0,2,1} %get-tuple-element.2904, f32[256]{0} %get-tuple-element.4319, f32[256]{0} %get-tuple-element.4320, f32[256]{0} %fusion.3786, f32[128,24,24,256]{3,0,2,1} %get-tuple-element.3835, f...\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 72.00M\n     Operator: op_type=\"Relu\" op_name=\"tpu_139858372878176/resnext50/conv1_relu/Relu\"\n     Shape: f32[128,48,48,64]{0,3,2,1}\n     Unpadded size: 72.00M\n     XLA label: %fusion.369 = f32[128,48,48,64]{0,3,2,1} fusion(f32[64]{0} %get-tuple-element.4285, f32[64]{0} %get-tuple-element.4284, f32[64]{0} %fusion.4745, f32[128,48,48,64]{0,3,2,1} %get-tuple-element.3828, f32[64]{0} %get-tuple-element.3947), kind=kLoop, calls=%fus...\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 36.00M\n     Operator: op_type=\"Reshape\" op_name=\"tpu_139858372878176/reshape_5/Reshape\"\n     Shape: f32[128,24,24,128]{3,0,2,1}\n     Unpadded size: 36.00M\n     XLA label: %reshape.1751 = f32[128,24,24,128]{3,0,2,1} reshape(f32[128,24,24,32,1,4]{5,3,0,2,1,4} %get-tuple-element.2704), metadata={op_type=\"Reshape\" op_name=\"tpu_139858372878176/reshape_5/Reshape\"}\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 36.00M\n     Operator: op_type=\"Reshape\" op_name=\"tpu_139858372878176/resnext50/reshape_3/Reshape\"\n     Shape: f32[128,24,24,128]{3,0,2,1}\n     Unpadded size: 36.00M\n     XLA label: %reshape.1747 = f32[128,24,24,128]{3,0,2,1} reshape(f32[128,24,24,32,1,4]{5,3,0,2,1,4} %get-tuple-element.2819), metadata={op_type=\"Reshape\" op_name=\"tpu_139858372878176/resnext50/reshape_3/Reshape\"}\n     Allocation type: HLO temp\n     ==========================\n\n  9. Size: 36.00M\n     Operator: op_type=\"Reshape\" op_name=\"tpu_139858372878176/reshape_3/Reshape\"\n     Shape: f32[128,24,24,128]{3,0,2,1}\n     Unpadded size: 36.00M\n     XLA label: %reshape.1750 = f32[128,24,24,128]{3,0,2,1} reshape(f32[128,24,24,32,1,4]{5,3,0,2,1,4} %get-tuple-element.2732), metadata={op_type=\"Reshape\" op_name=\"tpu_139858372878176/reshape_3/Reshape\"}\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 36.00M\n     Operator: op_type=\"Reshape\" op_name=\"tpu_139858372878176/resnext50/reshape_1/Reshape\"\n     Shape: f32[128,24,24,128]{3,0,2,1}\n     Unpadded size: 36.00M\n     XLA label: %reshape.1746 = f32[128,24,24,128]{3,0,2,1} reshape(f32[128,24,24,32,1,4]{5,3,0,2,1,4} %get-tuple-element.2820), metadata={op_type=\"Reshape\" op_name=\"tpu_139858372878176/resnext50/reshape_1/Reshape\"}\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 9.00M\n     Operator: op_type=\"InfeedDequeueTuple\" op_name=\"infeed-train\"\n     Shape: bf16[128,96,96,3]{0,3,2,1}\n     Unpadded size: 6.75M\n     Extra memory due to padding: 2.25M (1.3x expansion)\n     XLA label: %copy.1455 = bf16[128,96,96,3]{0,3,2,1} copy(bf16[128,96,96,3]{3,2,1,0} %reshape.1117), metadata={op_type=\"InfeedDequeueTuple\" op_name=\"infeed-train\"}\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 340.0K\n     Shape: (f32[1]{0}, f32[22528,1]{1,0}, f32[64]{0}, f32[64]{0}, f32[7,7,3,64]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,64,256]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,64,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[3,3,128,4]{3,2,1,0}, f32[256]{0}, f32[1,1,128,256]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,256,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[3,3,128,4]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,128,256]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,256,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[3,3,128,4]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,128,256]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,256,512]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,256,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[3,3,256,8]{3,2,1,0}, f32[512]{0}, f32[1,1,256,512]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,512,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[3,3,256,8]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,256,512]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,512,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[3,3,256,8]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,256,512]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,512,256]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[3,3,256,8]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,256,512]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,512,1024]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,512,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[3,3,512,16]{3,2,1,0}, f32[1024]{0}, f32[1,1,512,1024]{3,2,1,0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 340.0K\n     XLA label: %cross-replica-sum.165 = (f32[1]{0}, f32[22528,1]{1,0}, f32[64]{0}, f32[64]{0}, f32[7,7,3,64]{3,2,1,0}, f32[256]{0}, f32[256]{0}, f32[1,1,64,256]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[1,1,64,128]{3,2,1,0}, f32[128]{0}, f32[128]{0}, f32[3,3,128,4]{3,2,1,0...\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 168.0K\n     Shape: (f32[1,1,1024,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[3,3,512,16]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,512,1024]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,1024,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[3,3,512,16]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,512,1024]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,1024,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[3,3,512,16]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,512,1024]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,1024,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[3,3,512,16]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,512,1024]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,1024,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[3,3,512,16]{3,2,1,0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 168.0K\n     XLA label: %cross-replica-sum.166 = (f32[1,1,1024,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[3,3,512,16]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,512,1024]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[1,1,1024,512]{3,2,1,0}, f32[512]{0}, f32[512]{0}, f32[3,3,512,16...\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 128.0K\n     Shape: s32[32768]{0}\n     Unpadded size: 128.0K\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  15. Size: 60.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block1_0_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 60.0K\n     XLA label: %fusion.4217 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.4339, f...\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 60.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv5_block1_0_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0})\n     Unpadded size: 60.0K\n     XLA label: %fusion.3853 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}) fusion(f32[2048]{0} %get-tuple...\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 60.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block1_0_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 60.0K\n     XLA label: %fusion.4652 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.4289, f...\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 56.0K\n     Shape: (f32[1,1,512,1024]{3,2,1,0}, f32[2048]{0}, f32[2048]{0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,1024,1024]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[3,3,1024,32]{3,2,1,0}, f32[2048]{0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 56.0K\n     XLA label: %cross-replica-sum.167 = (f32[1,1,512,1024]{3,2,1,0}, f32[2048]{0}, f32[2048]{0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,1024,1024]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[3,3,1024,32]{3,2,1,0}, f32[2048]{0}, f32[1,1,1024,2048]{...\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block1_1_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4239 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.4410, f32[] %divide.118, f32[512]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block4_3_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4033 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.4465, f32[] %divide.118, f32[1024...\n     Allocation type: HLO temp\n     ==========================\n\n  21. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block1_3_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.2270 = (f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024...\n     Allocation type: HLO temp\n     ==========================\n\n  22. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block4_1_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4261 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.4454, f32[] %divide.118, f32[512]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  23. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block2_1_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4247 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.4425, f32[] %divide.118, f32[512]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  24. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block2_1_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.2248 = (f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512...\n     Allocation type: HLO temp\n     ==========================\n\n  25. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block3_1_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4253 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.4439, f32[] %divide.118, f32[512]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  26. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block3_3_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4029 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.4450, f32[] %divide.118, f32[1024...\n     Allocation type: HLO temp\n     ==========================\n\n  27. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block3_2_conv/depthwise_kernel/0/ResourceApplyAdam\"\n     Shape: (f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.2204 = (f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,...\n     Allocation type: HLO temp\n     ==========================\n\n  28. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block3_3_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4027 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.4449, f32[] %divide.118, f32[1024...\n     Allocation type: HLO temp\n     ==========================\n\n  29. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block1_0_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4015 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.4404, f32[] %divide.118, f32[1024...\n     Allocation type: HLO temp\n     ==========================\n\n  30. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block5_1_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4269 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.4469, f32[] %divide.118, f32[512]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  31. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv5_block1_0_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.1839 = (f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1,1,...\n     Allocation type: HLO temp\n     ==========================\n\n  32. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block1_0_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4654 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.4290, f32[] %divide.118, f32[256]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  33. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block2_2_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4562 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.4314, f32[] %divide.118, f32[128]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  34. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block2_3_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4660 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.4319, f32[] %divide.118, f32[256]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  35. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block1_2_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4672 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.4349, f32[] %divide.118, f32[256]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  36. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block1_2_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4674 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.4350, f32[] %divide.118, f32[256]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  37. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block1_2_conv/depthwise_kernel/0/ResourceApplyAdam\"\n     Shape: (f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.2355 = (f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{3,2,1,0}, f32[3,3,256,8]{...\n     Allocation type: HLO temp\n     ==========================\n\n  38. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block1_3_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.2484 = (f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0...\n     Allocation type: HLO temp\n     ==========================\n\n  39. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block2_3_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4225 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.4369, f32[] %divide.118, f32[512]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  40. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block2_3_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4227 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.4370, f32[] %divide.118, f32[512]{0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  41. Size: 48.0K\n     Shape: (f32[1,1,2048,1024]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[3,3,1024,32]{3,2,1,0}, f32[2048]{0}, f32[2048]{0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,2048,1024]{3,2,1,0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 48.0K\n     XLA label: %cross-replica-sum.168 = (f32[1,1,2048,1024]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[3,3,1024,32]{3,2,1,0}, f32[2048]{0}, f32[2048]{0}, f32[1,1,1024,2048]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}, f32[1,1,2048,1024]{3,2,1,0}, f32[1024]{0}, f32[1024]{0}) cros...\n     Allocation type: HLO temp\n     ==========================\n\n  42. Size: 48.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block1_0_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 48.0K\n     XLA label: %fusion.4017 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.4405, f32[] %divide.118, f32[1024...\n     Allocation type: HLO temp\n     ==========================\n\n  43. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv5_block2_1_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.4051 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.4519, f32[] %divide.118, f32[1024]{0} %get-tuple-element.4834, f32[1024]{0}...\n     Allocation type: HLO temp\n     ==========================\n\n  44. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block1_2_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.4554 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.4299, f32[] %divide.118, f32[128]{0} %get-tuple-element.4570, f32[128]{0} %get-tuple-...\n     Allocation type: HLO temp\n     ==========================\n\n  45. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv5_block1_3_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.3859 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}) fusion(f32[2048]{0} %get-tuple-element.4515, f32[] %divide.118, f32[2048]{0} %get-tuple-element.4830, f32[2048]{0}...\n     Allocation type: HLO temp\n     ==========================\n\n  46. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv5_block1_2_conv/depthwise_kernel/0/ResourceApplyAdam\"\n     Shape: (f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.2050 = (f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0}, f32[3,3,1024,32]{3,2,1,0...\n     Allocation type: HLO temp\n     ==========================\n\n  47. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block1_3_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.4021 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.4420, f32[] %divide.118, f32[1024]{0} %get-tuple-element.4716, f32[1024]{0}...\n     Allocation type: HLO temp\n     ==========================\n\n  48. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block1_0_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.2268 = (f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024]{3,2,1,0}, f32[1,1,512,1024...\n     Allocation type: HLO temp\n     ==========================\n\n  49. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block2_1_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.2470 = (f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0}, f32[1,1,512,256]{3,2,1,0...\n     Allocation type: HLO temp\n     ==========================\n\n  50. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block1_3_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.2776 = (f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0}, f32[1,1,128,256]{3,2,1,0...\n     Allocation type: HLO temp\n     ==========================\n\n  51. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block1_1_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.4670 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.4345, f32[] %divide.118, f32[256]{0} %get-tuple-element.4625, f32[256]{0} %get-tuple-...\n     Allocation type: HLO temp\n     ==========================\n\n  52. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block1_1_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.4550 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.4294, f32[] %divide.118, f32[128]{0} %get-tuple-element.4564, f32[128]{0} %get-tuple-...\n     Allocation type: HLO temp\n     ==========================\n\n  53. Size: 36.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block1_2_conv/depthwise_kernel/0/ResourceApplyAdam\"\n     Shape: (f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0})\n     Unpadded size: 36.0K\n     XLA label: %fusion.2435 = (f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}, f32[3,3,128,4]{3,2,1,0}) fusion(f32[3,3,...\n     Allocation type: HLO temp\n     ==========================\n\n  54. Size: 32.0K\n     Shape: s32[8192]{0}\n     Unpadded size: 32.0K\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  55. Size: 24.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5309 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.4448, f32[1024]{0} %get-tuple-element.2005, f32[1024]{0} %get-tuple-element.1946, f32[1024]{0} %get-tuple-element.200...\n     Allocation type: HLO temp\n     ==========================\n\n  56. Size: 24.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv5_block2_1_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,2048,1024]{3,2,1,0}, f32[1,1,2048,1024]{3,2,1,0}, f32[1,1,2048,1024]{3,2,1,0}, f32[1,1,2048,1024]{3,2,1,0}, f32[1,1,2048,1024]{3,2,1,0}, f32[1,1,2048,1024]{3,2,1,0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.1831 = (f32[1,1,2048,1024]{3,2,1,0}, f32[1,1,2048,1024]{3,2,1,0}, f32[1,1,2048,1024]{3,2,1,0}, f32[1,1,2048,1024]{3,2,1,0}, f32[1,1,2048,1024]{3,2,1,0}, f32[1,1,2048,1024]{3,2,1,0}) fusion(f32[1,1,2048,1024]{3,2,1,0} %copy.2183, f32[] %divide.118, ...\n     Allocation type: HLO temp\n     ==========================\n\n  57. Size: 24.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5310 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.2034, f32[512]{0} %get-tuple-element.2033, f32[512]{0} %get-tuple-element.4452, f32[512]{0} %get-tuple-element.2075, f32[512...\n     Allocation type: HLO temp\n     ==========================\n\n  58. Size: 24.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5312 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.2073, f32[512]{0} %get-tuple-element.2072, f32[512]{0} %get-tuple-element.4467, f32[512]{0} %get-tuple-element.2054, f32[512...\n     Allocation type: HLO temp\n     ==========================\n\n  59. Size: 24.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5313 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1992, f32[1024]{0} %get-tuple-element.1991, f32[1024]{0} %get-tuple-element.4477, f32[1024]{0} %get-tuple-element.200...\n     Allocation type: HLO temp\n     ==========================\n\n  60. Size: 24.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5314 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.2058, f32[512]{0} %get-tuple-element.2057, f32[512]{0} %get-tuple-element.4482, f32[512]{0} %get-tuple-element.2048, f32[512...\n     Allocation type: HLO temp\n     ==========================\n\n  61. Size: 24.0K\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5316 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}) fusion(f32[2048]{0} %get-tuple-element.4498, f32[2048]{0} %get-tuple-element.2029, f32[2048]{0} %get-tuple-element.2037, f32[2048]{0} %get-tuple-element.203...\n     Allocation type: HLO temp\n     ==========================\n\n  62. Size: 24.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5317 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1983, f32[1024]{0} %get-tuple-element.1982, f32[1024]{0} %get-tuple-element.4502, f32[1024]{0} %get-tuple-element.195...\n     Allocation type: HLO temp\n     ==========================\n\n  63. Size: 24.0K\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5318 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}) fusion(f32[2048]{0} %get-tuple-element.4513, f32[2048]{0} %get-tuple-element.2045, f32[2048]{0} %get-tuple-element.2063, f32[2048]{0} %get-tuple-element.204...\n     Allocation type: HLO temp\n     ==========================\n\n  64. Size: 24.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5319 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.2087, f32[1024]{0} %get-tuple-element.2086, f32[1024]{0} %get-tuple-element.4517, f32[1024]{0} %get-tuple-element.207...\n     Allocation type: HLO temp\n     ==========================\n\n  65. Size: 24.0K\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5320 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}) fusion(f32[2048]{0} %get-tuple-element.2042, f32[2048]{0} %get-tuple-element.2052, f32[2048]{0} %get-tuple-element.2051, f32[2048]{0} %get-tuple-element.452...\n     Allocation type: HLO temp\n     ==========================\n\n  66. Size: 24.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv1_bn/beta/0/ResourceApplyAdam\"\n     Shape: (f32[64]{0}, f32[64]{0}, f32[64]{0}, f32[64]{0}, f32[64]{0}, f32[64]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.4511 = (f32[64]{0}, f32[64]{0}, f32[64]{0}, f32[64]{0}, f32[64]{0}, f32[64]{0}) fusion(f32[64]{0} %get-tuple-element.4284, f32[] %divide.118, f32[64]{0} %get-tuple-element.4551, f32[64]{0} %get-tuple-element.2096, f32[64]{0} %get-tuple-element.4552...\n     Allocation type: HLO temp\n     ==========================\n\n  67. Size: 24.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block2_1_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.4560 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.4310, f32[] %divide.118, f32[128]{0} %get-tuple-element.4584, f32[128]{0} %get-tuple-element.2111, f32[128]{0} %get-tuple-el...\n     Allocation type: HLO temp\n     ==========================\n\n  68. Size: 24.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block2_1_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,256,128]{3,2,1,0}, f32[1,1,256,128]{3,2,1,0}, f32[1,1,256,128]{3,2,1,0}, f32[1,1,256,128]{3,2,1,0}, f32[1,1,256,128]{3,2,1,0}, f32[1,1,256,128]{3,2,1,0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.2768 = (f32[1,1,256,128]{3,2,1,0}, f32[1,1,256,128]{3,2,1,0}, f32[1,1,256,128]{3,2,1,0}, f32[1,1,256,128]{3,2,1,0}, f32[1,1,256,128]{3,2,1,0}, f32[1,1,256,128]{3,2,1,0}) fusion(f32[1,1,256,128]{3,2,1,0} %get-tuple-element.4311, f32[] %divide.118, f...\n     Allocation type: HLO temp\n     ==========================\n\n  69. Size: 24.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block1_2_conv/depthwise_kernel/0/ResourceApplyAdam\"\n     Shape: (f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.2200 = (f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}, f32[3,3,512,16]{3,2,1,0}) fusion(f32[3,3,512,16]{3,2,1,0} %reshape.76.remat, f32[] %divide.118, f32[3,3,512,16...\n     Allocation type: HLO temp\n     ==========================\n\n  70. Size: 24.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5302 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1968, f32[1024]{0} %get-tuple-element.1969, f32[1024]{0} %get-tuple-element.4402, f32[1024]{0} %get-tuple-element.197...\n     Allocation type: HLO temp\n     ==========================\n\n  71. Size: 24.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5281 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1846, f32[256]{0} %get-tuple-element.1845, f32[256]{0} %get-tuple-element.4302, f32[256]{0} %get-tuple-element.1833, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  72. Size: 24.0K\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5283 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.1891, f32[128]{0} %get-tuple-element.1892, f32[128]{0} %get-tuple-element.4307, f32[128]{0} %get-tuple-element.1890, f32[128...\n     Allocation type: HLO temp\n     ==========================\n\n  73. Size: 24.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5285 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1867, f32[256]{0} %get-tuple-element.1868, f32[256]{0} %get-tuple-element.1827, f32[256]{0} %get-tuple-element.4317, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  74. Size: 24.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5288 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1843, f32[256]{0} %get-tuple-element.1844, f32[256]{0} %get-tuple-element.1829, f32[256]{0} %get-tuple-element.4332, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  75. Size: 24.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5289 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.1901, f32[512]{0} %get-tuple-element.1902, f32[512]{0} %get-tuple-element.1886, f32[512]{0} %get-tuple-element.4337, f32[512...\n     Allocation type: HLO temp\n     ==========================\n\n  76. Size: 24.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5290 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1837, f32[256]{0} %get-tuple-element.1838, f32[256]{0} %get-tuple-element.4342, f32[256]{0} %get-tuple-element.1825, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  77. Size: 24.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5292 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.1883, f32[512]{0} %get-tuple-element.1884, f32[512]{0} %get-tuple-element.4352, f32[512]{0} %get-tuple-element.1882, f32[512...\n     Allocation type: HLO temp\n     ==========================\n\n  78. Size: 24.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5293 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1952, f32[256]{0} %get-tuple-element.1953, f32[256]{0} %get-tuple-element.2004, f32[256]{0} %get-tuple-element.4357, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  79. Size: 24.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5295 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.1879, f32[512]{0} %get-tuple-element.1880, f32[512]{0} %get-tuple-element.4368, f32[512]{0} %get-tuple-element.1921, f32[512...\n     Allocation type: HLO temp\n     ==========================\n\n  80. Size: 24.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5296 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1997, f32[256]{0} %get-tuple-element.1998, f32[256]{0} %get-tuple-element.4372, f32[256]{0} %get-tuple-element.1941, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  81. Size: 24.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5298 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.1916, f32[512]{0} %get-tuple-element.1917, f32[512]{0} %get-tuple-element.4382, f32[512]{0} %get-tuple-element.1929, f32[512...\n     Allocation type: HLO temp\n     ==========================\n\n  82. Size: 24.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5301 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.4398, f32[512]{0} %get-tuple-element.1926, f32[512]{0} %get-tuple-element.1903, f32[512]{0} %get-tuple-element.1927, f32[512...\n     Allocation type: HLO temp\n     ==========================\n\n  83. Size: 24.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 24.0K\n     XLA label: %fusion.5306 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.2070, f32[512]{0} %get-tuple-element.2044, f32[512]{0} %get-tuple-element.2069, f32[512]{0} %get-tuple-element.4422, f32[512...\n     Allocation type: HLO temp\n     ==========================\n\n  84. Size: 20.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.5308 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.2024, f32[512]{0} %get-tuple-element.2023, f32[512]{0} %get-tuple-element.4437, f32[512]{0} %get-tuple-element.2056, f32[512]{0} %get-tup...\n     Allocation type: HLO temp\n     ==========================\n\n  85. Size: 20.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block1_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.4840 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.1818, f32[128]{0} %get-tuple-element.1923, f32[128]{0} %get-tuple-element.1922, f32[128]{0} %get-tuple-element.4293, f32[128]{0} %get-tup...\n     Allocation type: HLO temp\n     ==========================\n\n  86. Size: 20.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv5_block3_3_bn/FusedBatchNorm\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.3481 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[2048]{0}) fusion(f32[2048]{0} %get-tuple-element.2040, f32[2048]{0} %get-tuple-element.2039, f32[2048]{0} %get-tuple-element.4542, f32[2048]{0} %get-tuple-element.2028, f32[2048]{0...\n     Allocation type: HLO temp\n     ==========================\n\n  87. Size: 20.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.5322 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %reshape.1633, f32[1024]{0} %reshape.1632, f32[1024]{0} %get-tuple-element.4537, f32[1024]{0} %reshape.1680, f32[1024]{0} %get-tuple-element.4538, f32...\n     Allocation type: HLO temp\n     ==========================\n\n  88. Size: 20.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.5321 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.2089, f32[1024]{0} %get-tuple-element.2088, f32[1024]{0} %get-tuple-element.2083, f32[1024]{0} %get-tuple-element.4533, f32[1024]{0...\n     Allocation type: HLO temp\n     ==========================\n\n  89. Size: 20.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.5315 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1987, f32[1024]{0} %get-tuple-element.1986, f32[1024]{0} %get-tuple-element.4492, f32[1024]{0} %get-tuple-element.1996, f32[1024]{0...\n     Allocation type: HLO temp\n     ==========================\n\n  90. Size: 20.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.5311 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1979, f32[1024]{0} %get-tuple-element.1978, f32[1024]{0} %get-tuple-element.4462, f32[1024]{0} %get-tuple-element.1967, f32[1024]{0...\n     Allocation type: HLO temp\n     ==========================\n\n  91. Size: 20.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.5307 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1943, f32[1024]{0} %get-tuple-element.1942, f32[1024]{0} %get-tuple-element.4432, f32[1024]{0} %get-tuple-element.1951, f32[1024]{0...\n     Allocation type: HLO temp\n     ==========================\n\n  92. Size: 20.0K\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.5305 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1958, f32[1024]{0} %get-tuple-element.1959, f32[1024]{0} %get-tuple-element.4417, f32[1024]{0} %get-tuple-element.2010, f32[1024]{0...\n     Allocation type: HLO temp\n     ==========================\n\n  93. Size: 20.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.5303 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.1895, f32[512]{0} %get-tuple-element.1896, f32[512]{0} %get-tuple-element.4407, f32[512]{0} %get-tuple-element.1925, f32[512]{0} %get-tup...\n     Allocation type: HLO temp\n     ==========================\n\n  94. Size: 20.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.5299 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1984, f32[256]{0} %get-tuple-element.1985, f32[256]{0} %get-tuple-element.4387, f32[256]{0} %get-tuple-element.2012, f32[256]{0} %get-tup...\n     Allocation type: HLO temp\n     ==========================\n\n  95. Size: 20.0K\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0})\n     Unpadded size: 20.0K\n     XLA label: %fusion.5286 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.1918, f32[128]{0} %get-tuple-element.1919, f32[128]{0} %get-tuple-element.4322, f32[128]{0} %get-tuple-element.1888, f32[128]{0} %get-tup...\n     Allocation type: HLO temp\n     ==========================\n\n  96. Size: 16.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block1_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 16.0K\n     XLA label: %fusion.1769 = (f32[2048]{0}, f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[128,3,3,2048]{3,0,2,1} %fusion.2144.remat, f32[2048]{0} %get-tuple-element.4244, f32[128,3,3,2048]{3,0,2,1} %convolution.43.remat, f32[2048]{0} %get-tuple-elem...\n     Allocation type: HLO temp\n     ==========================\n\n  97. Size: 16.0K\n     Shape: (f32[3,3,1024,32]{3,2,1,0}, f32[2048]{0}, f32[2048]{0}, f32[1,1,1024,2048]{3,2,1,0})\n     Unpadded size: 16.0K\n     XLA label: %cross-replica-sum.169 = (f32[3,3,1024,32]{3,2,1,0}, f32[2048]{0}, f32[2048]{0}, f32[1,1,1024,2048]{3,2,1,0}) cross-replica-sum(f32[3,3,1024,32]{3,2,1,0} %reshape.998, f32[2048]{0} %get-tuple-element.2026, f32[2048]{0} %fusion.4891, f32[1,1,1024,2048]{3,2,...\n     Allocation type: HLO temp\n     ==========================\n\n  98. Size: 16.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[64]{0}, f32[64]{0}, f32[64]{0}, f32[64]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.4839 = (f32[64]{0}, f32[64]{0}, f32[64]{0}, f32[64]{0}) fusion(f32[64]{0} %get-tuple-element.1810, f32[64]{0} %get-tuple-element.1831, f32[64]{0} %get-tuple-element.1830, f32[64]{0} %get-tuple-element.4283, f32[64]{0} %get-tuple-element.4282), kind...\n     Allocation type: HLO temp\n     ==========================\n\n  99. Size: 16.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block1_0_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.4846 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1834, f32[256]{0} %get-tuple-element.1850, f32[256]{0} %get-tuple-element.1849, f32[256]{0} %get-tuple-element.4288, f32[256]{0} %get-tuple-element.42...\n     Allocation type: HLO temp\n     ==========================\n\n  100. Size: 16.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block1_0_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 16.0K\n     XLA label: %fusion.621 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1} %convolution.1.remat2, f32[256]{0} %get-tuple-element.3970, f32[128,24,24,256]{3,0,2,1} %fusion.1024.remat2, f32[256]{0} %get-tuple-elemen...\n     Allocation type: HLO temp\n     ==========================\n\n  101. Size: 16.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block1_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 16.0K\n     XLA label: %fusion.892 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[128,12,12,512]{3,0,2,1} %fusion.1507.remat, f32[512]{0} %get-tuple-element.4048, f32[128,12,12,512]{3,0,2,1} %convolution.11.remat, f32[512]{0} %get-tuple-element...\n     Allocation type: HLO temp\n     ==========================\n\n  102. Size: 16.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block1_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 16.0K\n     XLA label: %fusion.1370 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.1948.remat, f32[1024]{0} %get-tuple-element.4152, f32[128,6,6,1024]{3,0,2,1} %convolution.24.remat, f32[1024]{0} %get-tuple-elem...\n     Allocation type: HLO temp\n     ==========================\n\n  103. Size: 16.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv5_block2_2_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.3017 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %reshape.1678, f32[1024]{0} %reshape.1677, f32[1024]{0} %get-tuple-element.4523, f32[1024]{0} %reshape.1630, f32[1024]{0} %reshape.1629, f32[1024]{0} %get-tuple-ele...\n     Allocation type: HLO temp\n     ==========================\n\n  104. Size: 16.0K\n     Operator: op_type=\"AssignSubVariableOp\" op_name=\"tpu_139858372878176/resnext50/conv5_block1_2_bn/AssignMovingAvg_1/AssignSubVariableOp\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.4169 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.4508, f32[1024]{0} %reshape.1627, f32[1024]{0} %reshape.1626, f32[1024]{0} %reshape.1675, f32[1024]{0} %reshape.1674, f32[1024]{0} %get-tuple-ele...\n     Allocation type: HLO temp\n     ==========================\n\n  105. Size: 16.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block6_2_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.3261 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %reshape.1672, f32[512]{0} %reshape.1671, f32[512]{0} %get-tuple-element.4488, f32[512]{0} %reshape.1624, f32[512]{0} %reshape.1623, f32[512]{0} %get-tuple-element.4487)...\n     Allocation type: HLO temp\n     ==========================\n\n  106. Size: 16.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block5_2_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.3263 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %reshape.1669, f32[512]{0} %reshape.1668, f32[512]{0} %get-tuple-element.4473, f32[512]{0} %reshape.1621, f32[512]{0} %reshape.1620, f32[512]{0} %get-tuple-element.4472)...\n     Allocation type: HLO temp\n     ==========================\n\n  107. Size: 16.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block3_2_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.3267 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %reshape.1663, f32[512]{0} %reshape.1662, f32[512]{0} %get-tuple-element.4443, f32[512]{0} %reshape.1615, f32[512]{0} %reshape.1614, f32[512]{0} %get-tuple-element.4442)...\n     Allocation type: HLO temp\n     ==========================\n\n  108. Size: 16.0K\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.5280 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %reshape.1588, f32[128]{0} %reshape.1587, f32[128]{0} %get-tuple-element.4298, f32[128]{0} %get-tuple-element.4297), kind=kLoop, calls=%fused_computation.3680.clone.clon...\n     Allocation type: HLO temp\n     ==========================\n\n  109. Size: 16.0K\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.5284 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %reshape.1591, f32[128]{0} %reshape.1590, f32[128]{0} %get-tuple-element.4312, f32[128]{0} %reshape.1638, f32[128]{0} %reshape.1639, f32[128]{0} %get-tuple-element.4313)...\n     Allocation type: HLO temp\n     ==========================\n\n  110. Size: 16.0K\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.5287 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %reshape.1594, f32[128]{0} %reshape.1593, f32[128]{0} %get-tuple-element.4327, f32[128]{0} %reshape.1641, f32[128]{0} %get-tuple-element.4328, f32[128]{0} %reshape.1642)...\n     Allocation type: HLO temp\n     ==========================\n\n  111. Size: 16.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.5291 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %reshape.1597, f32[256]{0} %reshape.1596, f32[256]{0} %get-tuple-element.4347, f32[256]{0} %reshape.1644, f32[256]{0} %get-tuple-element.4348, f32[256]{0} %reshape.1645)...\n     Allocation type: HLO temp\n     ==========================\n\n  112. Size: 16.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.5294 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %reshape.1600, f32[256]{0} %reshape.1599, f32[256]{0} %get-tuple-element.4362, f32[256]{0} %reshape.1647, f32[256]{0} %get-tuple-element.4363, f32[256]{0} %reshape.1648)...\n     Allocation type: HLO temp\n     ==========================\n\n  113. Size: 16.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.5297 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %reshape.1603, f32[256]{0} %reshape.1602, f32[256]{0} %get-tuple-element.4377, f32[256]{0} %reshape.1650, f32[256]{0} %get-tuple-element.4378, f32[256]{0} %reshape.1651)...\n     Allocation type: HLO temp\n     ==========================\n\n  114. Size: 16.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block4_2_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.3265 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %reshape.1666, f32[512]{0} %reshape.1665, f32[512]{0} %get-tuple-element.4458, f32[512]{0} %reshape.1618, f32[512]{0} %reshape.1617, f32[512]{0} %get-tuple-element.4457)...\n     Allocation type: HLO temp\n     ==========================\n\n  115. Size: 16.0K\n     Shape: (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.5300 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %reshape.1606, f32[256]{0} %reshape.1605, f32[256]{0} %get-tuple-element.4392, f32[256]{0} %reshape.1653, f32[256]{0} %get-tuple-element.4393, f32[256]{0} %reshape.1654)...\n     Allocation type: HLO temp\n     ==========================\n\n  116. Size: 16.0K\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.5304 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %reshape.1609, f32[512]{0} %reshape.1608, f32[512]{0} %get-tuple-element.4412, f32[512]{0} %reshape.1656, f32[512]{0} %get-tuple-element.4413, f32[512]{0} %reshape.1657)...\n     Allocation type: HLO temp\n     ==========================\n\n  117. Size: 16.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block2_2_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 16.0K\n     XLA label: %fusion.3269 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %reshape.1660, f32[512]{0} %reshape.1659, f32[512]{0} %get-tuple-element.4428, f32[512]{0} %reshape.1612, f32[512]{0} %reshape.1611, f32[512]{0} %get-tuple-element.4427)...\n     Allocation type: HLO temp\n     ==========================\n\n  118. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block1_0_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1399 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[128,12,12,512]{3,0,2,1} %fusion.162, f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4406), kind=kOutput, calls=%fused_computation.1287, metadata={op_type=\"FusedBatchNorm\" op_...\n     Allocation type: HLO temp\n     ==========================\n\n  119. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv3_block4_3_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.905 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4401, f32[256]{0} %get-tuple-element.4394, f32[256]{0} %get-tuple-element.4395, f32[256]{0} %fusion.3777, f32[128,12,12,256]{3,0,2,1} ...\n     Allocation type: HLO temp\n     ==========================\n\n  120. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv3_block4_3_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.859 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4401, f32[256]{0} %get-tuple-element.4394, f32[256]{0} %get-tuple-element.4395, f32[256]{0} %get-tuple-element.4118, f32[128,12,12,256...\n     Allocation type: HLO temp\n     ==========================\n\n  121. Size: 12.0K\n     Shape: (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.184 = (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4}) fusion(f32[128,12,12,32,8,8]{5,3,0,2,1,4} %copy.1732), kind=kLoop, calls=%fused_computation.178\n     Allocation type: HLO temp\n     ==========================\n\n  122. Size: 12.0K\n     Shape: (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.197 = (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4}) fusion(f32[128,12,12,32,8,8]{5,3,0,2,1,4} %copy.1485), kind=kLoop, calls=%fused_computation.191\n     Allocation type: HLO temp\n     ==========================\n\n  123. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv3_block4_1_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1340 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[128,12,12,512]{3,0,2,1} %fusion.145, f32[1,1,512,256]{3,2,1,0} %get-tuple-element.4391), kind=kOutput, calls=%fused_computation.1234, metadata={op_type=\"FusedBatchNorm\" op_na...\n     Allocation type: HLO temp\n     ==========================\n\n  124. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv3_block4_1_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1402 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[128,12,12,512]{3,0,2,1} %fusion.161, f32[1,1,512,256]{3,2,1,0} %get-tuple-element.4391), kind=kOutput, calls=%fused_computation.1290, metadata={op_type=\"FusedBatchNorm\" op_na...\n     Allocation type: HLO temp\n     ==========================\n\n  125. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv3_block3_3_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.861 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4386, f32[256]{0} %get-tuple-element.4379, f32[256]{0} %get-tuple-element.4380, f32[256]{0} %get-tuple-element.4095, f32[128,12,12,256...\n     Allocation type: HLO temp\n     ==========================\n\n  126. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv3_block3_3_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.907 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4386, f32[256]{0} %get-tuple-element.4379, f32[256]{0} %get-tuple-element.4380, f32[256]{0} %fusion.3779, f32[128,12,12,256]{3,0,2,1} ...\n     Allocation type: HLO temp\n     ==========================\n\n  127. Size: 12.0K\n     Shape: (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.188 = (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4}) fusion(f32[128,12,12,32,8,8]{5,3,0,2,1,4} %copy.1723), kind=kLoop, calls=%fused_computation.182\n     Allocation type: HLO temp\n     ==========================\n\n  128. Size: 12.0K\n     Shape: (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.198 = (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4}) fusion(f32[128,12,12,32,8,8]{5,3,0,2,1,4} %copy.1477), kind=kLoop, calls=%fused_computation.192\n     Allocation type: HLO temp\n     ==========================\n\n  129. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv3_block3_1_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1345 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[1,1,512,256]{3,2,1,0} %get-tuple-element.4376, f32[512]{0} %get-tuple-element.4369, f32[512]{0} %get-tuple-element.4370, f32[512]{0} %get-tuple-element.4072, f32[128,12,12,51...\n     Allocation type: HLO temp\n     ==========================\n\n  130. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv3_block3_1_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1405 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[128,12,12,512]{3,0,2,1} %fusion.160, f32[1,1,512,256]{3,2,1,0} %get-tuple-element.4376), kind=kOutput, calls=%fused_computation.1293, metadata={op_type=\"FusedBatchNorm\" op_na...\n     Allocation type: HLO temp\n     ==========================\n\n  131. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv3_block2_3_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.909 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4371, f32[256]{0} %get-tuple-element.4364, f32[256]{0} %get-tuple-element.4365, f32[256]{0} %fusion.3781, f32[128,12,12,256]{3,0,2,1} ...\n     Allocation type: HLO temp\n     ==========================\n\n  132. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block3_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1736 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[1,1,1024,512]{3,2,1,0} %get-tuple-element.4441, f32[1024]{0} %get-tuple-element.4434, f32[1024]{0} %get-tuple-element.4435, f32[1024]{0} %get-tuple-element.4173, f32[128,6,6,10...\n     Allocation type: HLO temp\n     ==========================\n\n  133. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv3_block2_3_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.863 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4371, f32[256]{0} %get-tuple-element.4364, f32[256]{0} %get-tuple-element.4365, f32[256]{0} %get-tuple-element.4071, f32[128,12,12,256...\n     Allocation type: HLO temp\n     ==========================\n\n  134. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block1_0_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1337 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4406, f32[128,12,12,512]{3,0,2,1} %fusion.145, f32[512]{0} %get-tuple-element.4399, f32[512]{0} %get-tuple-element.4400, f32[512]{0...\n     Allocation type: HLO temp\n     ==========================\n\n  135. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block1_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.903 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[128,12,12,512]{3,0,2,1} %fusion.162, f32[1,1,512,512]{3,2,1,0} %get-tuple-element.4411), kind=kOutput, calls=%fused_computation.856, metadata={op_type=\"FusedBatchNorm\" op_name...\n     Allocation type: HLO temp\n     ==========================\n\n  136. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block1_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.856 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[1,1,512,512]{3,2,1,0} %get-tuple-element.4411, f32[128,12,12,512]{3,0,2,1} %fusion.145, f32[512]{0} %get-tuple-element.4399, f32[512]{0} %get-tuple-element.4400, f32[512]{0} %...\n     Allocation type: HLO temp\n     ==========================\n\n  137. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,4,3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.366 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,4,3,0,2,1}) fusion(f32[128,6,6,32,16,16]{5,4,3,0,2,1} %reshape.78), kind=kLoop, calls=%fused_computation.348\n     Allocation type: HLO temp\n     ==========================\n\n  138. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.583 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1741), kind=kLoop, calls=%fused_computation.562\n     Allocation type: HLO temp\n     ==========================\n\n  139. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block1_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1397 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4421, f32[512]{0} %get-tuple-element.4414, f32[512]{0} %get-tuple-element.4415, f32[512]{0} %fusion.3288, f32[128,6,6,512]{3,0,2,1}...\n     Allocation type: HLO temp\n     ==========================\n\n  140. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block1_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1336 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4421, f32[512]{0} %get-tuple-element.4414, f32[512]{0} %get-tuple-element.4415, f32[512]{0} %get-tuple-element.4150, f32[128,6,6,51...\n     Allocation type: HLO temp\n     ==========================\n\n  141. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block2_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1797 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.250, f32[1,1,1024,512]{3,2,1,0} %copy.2088), kind=kOutput, calls=%fused_computation.1612, metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_1398583...\n     Allocation type: HLO temp\n     ==========================\n\n  142. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block2_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1741 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[1,1,1024,512]{3,2,1,0} %copy.2088, f32[1024]{0} %get-tuple-element.4404, f32[1024]{0} %get-tuple-element.4419, f32[1024]{0} %get-tuple-element.4420, f32[1024]{0} %get-tuple-ele...\n     Allocation type: HLO temp\n     ==========================\n\n  143. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.575 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1758), kind=kLoop, calls=%fused_computation.554\n     Allocation type: HLO temp\n     ==========================\n\n  144. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.588 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1494), kind=kLoop, calls=%fused_computation.567\n     Allocation type: HLO temp\n     ==========================\n\n  145. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block2_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1395 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4436, f32[512]{0} %get-tuple-element.4429, f32[512]{0} %get-tuple-element.4430, f32[512]{0} %fusion.3286, f32[128,6,6,512]{3,0,2,1}...\n     Allocation type: HLO temp\n     ==========================\n\n  146. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block2_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1334 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4436, f32[512]{0} %get-tuple-element.4429, f32[512]{0} %get-tuple-element.4430, f32[512]{0} %get-tuple-element.3008, f32[128,6,6,51...\n     Allocation type: HLO temp\n     ==========================\n\n  147. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block3_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1794 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.251, f32[1,1,1024,512]{3,2,1,0} %get-tuple-element.4441), kind=kOutput, calls=%fused_computation.1609, metadata={op_type=\"FusedBatchNorm\" op_name...\n     Allocation type: HLO temp\n     ==========================\n\n  148. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block6_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1387 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4496, f32[512]{0} %get-tuple-element.4489, f32[512]{0} %get-tuple-element.4490, f32[512]{0} %fusion.3278, f32[128,6,6,512]{3,0,2,1}...\n     Allocation type: HLO temp\n     ==========================\n\n  149. Size: 12.0K\n     Shape: (f32[32,4]{1,0}, f32[32,4]{1,0}, f32[128,24,24,32,1,4]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.70 = (f32[32,4]{1,0}, f32[32,4]{1,0}, f32[128,24,24,32,1,4]{5,3,0,2,1,4}) fusion(f32[128,24,24,32,4,4]{5,3,0,2,1,4} %copy.1464), kind=kLoop, calls=%fused_computation.64\n     Allocation type: HLO temp\n     ==========================\n\n  150. Size: 12.0K\n     Shape: (s32[128]{0}, f32[3538944]{0}, f32[128]{0})\n     Unpadded size: 12.0K\n     XLA label: %infeed.1 = ((s32[128]{0}, f32[3538944]{0}, f32[128]{0}), token[]) infeed(token[] %after-all)\n     Allocation type: HLO temp\n     ==========================\n\n  151. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv1_bn/FusedBatchNorm\"\n     Shape: (f32[64]{0}, f32[64]{0}, f32[128,48,48,64]{0,3,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.638 = (f32[64]{0}, f32[64]{0}, f32[128,48,48,64]{0,3,2,1}) fusion(bf16[128,96,96,3]{0,3,2,1} %copy.1455, bf16[7,7,3,64]{3,2,1,0} %reshape.3), kind=kOutput, calls=%fused_computation.613, metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_13985837287817...\n     Allocation type: HLO temp\n     ==========================\n\n  152. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_1_bn/FusedBatchNorm\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.922 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(bf16[128,24,24,64]{0,3,2,1} %reduce-window, f32[1,1,64,128]{3,2,1,0} %get-tuple-element.4296), kind=kOutput, calls=%fused_computation.875, metadata={op_type=\"FusedBatchNorm\" op_na...\n     Allocation type: HLO temp\n     ==========================\n\n  153. Size: 12.0K\n     Shape: (f32[32,4]{1,0}, f32[32,4]{1,0}, f32[128,24,24,32,1,4]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.72 = (f32[32,4]{1,0}, f32[32,4]{1,0}, f32[128,24,24,32,1,4]{5,3,0,2,1,4}) fusion(f32[128,24,24,32,4,4]{5,3,0,2,1,4} %copy.1456), kind=kLoop, calls=%fused_computation.66\n     Allocation type: HLO temp\n     ==========================\n\n  154. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_3_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.634 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[1,1,128,256]{3,2,1,0} %get-tuple-element.4306, f32[128]{0} %get-tuple-element.4299, f32[128]{0} %get-tuple-element.4300, f32[128]{0} %fusion.3987, f32[128,24,24,128]{3,0,2,1} ...\n     Allocation type: HLO temp\n     ==========================\n\n  155. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block1_3_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.613 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[1,1,128,256]{3,2,1,0} %get-tuple-element.4306, f32[128]{0} %get-tuple-element.4299, f32[128]{0} %get-tuple-element.4300, f32[128]{0} %get-tuple-element.3955, f32[128,24,24,128...\n     Allocation type: HLO temp\n     ==========================\n\n  156. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_0_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.636 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(bf16[128,24,24,64]{0,3,2,1} %reduce-window.remat, f32[1,1,64,256]{3,2,1,0} %get-tuple-element.4291), kind=kOutput, calls=%fused_computation.611, metadata={op_type=\"FusedBatchNorm\"...\n     Allocation type: HLO temp\n     ==========================\n\n  157. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block2_1_bn/FusedBatchNorm\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.919 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1} %get-tuple-element.2904, f32[1,1,256,128]{3,2,1,0} %get-tuple-element.4311), kind=kOutput, calls=%fused_computation.872, metadata={op_type=\"FusedBatchN...\n     Allocation type: HLO temp\n     ==========================\n\n  158. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block2_1_bn/FusedBatchNorm\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.874 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1} %get-tuple-element.2903, f32[1,1,256,128]{3,2,1,0} %get-tuple-element.4311), kind=kOutput, calls=%fused_computation.829, metadata={op_type=\"FusedBatchN...\n     Allocation type: HLO temp\n     ==========================\n\n  159. Size: 12.0K\n     Shape: (f32[32,4]{1,0}, f32[32,4]{1,0}, f32[128,24,24,32,1,4]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.71 = (f32[32,4]{1,0}, f32[32,4]{1,0}, f32[128,24,24,32,1,4]{5,3,0,2,1,4}) fusion(f32[128,24,24,32,4,4]{5,3,0,2,1,4} %copy.1460), kind=kLoop, calls=%fused_computation.65\n     Allocation type: HLO temp\n     ==========================\n\n  160. Size: 12.0K\n     Shape: (f32[32,4]{1,0}, f32[32,4]{1,0}, f32[128,24,24,32,1,4]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.67 = (f32[32,4]{1,0}, f32[32,4]{1,0}, f32[128,24,24,32,1,4]{5,3,0,2,1,4}) fusion(f32[128,24,24,32,4,4]{5,3,0,2,1,4} %copy.1695), kind=kLoop, calls=%fused_computation.61\n     Allocation type: HLO temp\n     ==========================\n\n  161. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block2_3_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.632 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[1,1,128,256]{3,2,1,0} %get-tuple-element.4321, f32[128]{0} %get-tuple-element.4314, f32[128]{0} %get-tuple-element.4315, f32[128]{0} %fusion.3985, f32[128,24,24,128]{3,0,2,1} ...\n     Allocation type: HLO temp\n     ==========================\n\n  162. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block2_3_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.611 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[1,1,128,256]{3,2,1,0} %get-tuple-element.4321, f32[128]{0} %get-tuple-element.4314, f32[128]{0} %get-tuple-element.4315, f32[128]{0} %get-tuple-element.3987, f32[128,24,24,128...\n     Allocation type: HLO temp\n     ==========================\n\n  163. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block3_1_bn/FusedBatchNorm\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.916 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1} %fusion.105, f32[1,1,256,128]{3,2,1,0} %get-tuple-element.4326), kind=kOutput, calls=%fused_computation.869, metadata={op_type=\"FusedBatchNorm\" op_name...\n     Allocation type: HLO temp\n     ==========================\n\n  164. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block3_1_bn/FusedBatchNorm\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.869 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(f32[1,1,256,128]{3,2,1,0} %get-tuple-element.4326, f32[128,24,24,256]{3,0,2,1} %get-tuple-element.2903, f32[256]{0} %get-tuple-element.4319, f32[256]{0} %get-tuple-element.4320, f...\n     Allocation type: HLO temp\n     ==========================\n\n  165. Size: 12.0K\n     Shape: (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.192 = (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4}) fusion(f32[128,12,12,32,8,8]{5,3,0,2,1,4} %copy.1714), kind=kLoop, calls=%fused_computation.186\n     Allocation type: HLO temp\n     ==========================\n\n  166. Size: 12.0K\n     Shape: (f32[32,4]{1,0}, f32[32,4]{1,0}, f32[128,24,24,32,1,4]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.65 = (f32[32,4]{1,0}, f32[32,4]{1,0}, f32[128,24,24,32,1,4]{5,3,0,2,1,4}) fusion(f32[128,24,24,32,4,4]{5,3,0,2,1,4} %copy.1700), kind=kLoop, calls=%fused_computation.59\n     Allocation type: HLO temp\n     ==========================\n\n  167. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block3_3_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.630 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[1,1,128,256]{3,2,1,0} %get-tuple-element.4336, f32[128]{0} %get-tuple-element.4329, f32[128]{0} %get-tuple-element.4330, f32[128]{0} %fusion.3983, f32[128,24,24,128]{3,0,2,1} ...\n     Allocation type: HLO temp\n     ==========================\n\n  168. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block3_3_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.609 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[1,1,128,256]{3,2,1,0} %get-tuple-element.4336, f32[128]{0} %get-tuple-element.4329, f32[128]{0} %get-tuple-element.4330, f32[128]{0} %get-tuple-element.4014, f32[128,24,24,128...\n     Allocation type: HLO temp\n     ==========================\n\n  169. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv3_block1_0_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.913 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1} %fusion.106, f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4341), kind=kOutput, calls=%fused_computation.866, metadata={op_type=\"FusedBatchNorm\" op_name...\n     Allocation type: HLO temp\n     ==========================\n\n  170. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv3_block1_0_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.866 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4341, f32[256]{0} %get-tuple-element.4334, f32[256]{0} %get-tuple-element.4335, f32[256]{0} %get-tuple-element.4022, f32[128,24,24,256...\n     Allocation type: HLO temp\n     ==========================\n\n  171. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv3_block1_1_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.628 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1} %fusion.106, f32[1,1,256,256]{3,2,1,0} %get-tuple-element.4346), kind=kOutput, calls=%fused_computation.603, metadata={op_type=\"FusedBatchNorm\" op_name...\n     Allocation type: HLO temp\n     ==========================\n\n  172. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv3_block1_1_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.606 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[1,1,256,256]{3,2,1,0} %get-tuple-element.4346, f32[256]{0} %get-tuple-element.4334, f32[256]{0} %get-tuple-element.4335, f32[256]{0} %get-tuple-element.4022, f32[128,24,24,256...\n     Allocation type: HLO temp\n     ==========================\n\n  173. Size: 12.0K\n     Shape: (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,4,3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.110 = (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,4,3,0,2,1}) fusion(f32[128,12,12,32,8,8]{5,4,3,0,2,1} %reshape.30), kind=kLoop, calls=%fused_computation.104\n     Allocation type: HLO temp\n     ==========================\n\n  174. Size: 12.0K\n     Shape: (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.196 = (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4}) fusion(f32[128,12,12,32,8,8]{5,3,0,2,1,4} %copy.1705), kind=kLoop, calls=%fused_computation.190\n     Allocation type: HLO temp\n     ==========================\n\n  175. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv3_block1_3_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.911 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4356, f32[256]{0} %get-tuple-element.4349, f32[256]{0} %get-tuple-element.4350, f32[256]{0} %fusion.3783, f32[128,12,12,256]{3,0,2,1} ...\n     Allocation type: HLO temp\n     ==========================\n\n  176. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv3_block1_3_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.865 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4356, f32[256]{0} %get-tuple-element.4349, f32[256]{0} %get-tuple-element.4350, f32[256]{0} %get-tuple-element.4047, f32[128,12,12,256...\n     Allocation type: HLO temp\n     ==========================\n\n  177. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv3_block2_1_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1408 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[128,12,12,512]{3,0,2,1} %fusion.159, f32[1,1,512,256]{3,2,1,0} %get-tuple-element.4361), kind=kOutput, calls=%fused_computation.1296, metadata={op_type=\"FusedBatchNorm\" op_na...\n     Allocation type: HLO temp\n     ==========================\n\n  178. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv3_block2_1_bn/FusedBatchNorm\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1350 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[1,1,512,256]{3,2,1,0} %get-tuple-element.4361, f32[512]{0} %get-tuple-element.4339, f32[512]{0} %get-tuple-element.4354, f32[512]{0} %get-tuple-element.4355, f32[512]{0} %get...\n     Allocation type: HLO temp\n     ==========================\n\n  179. Size: 12.0K\n     Shape: (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.199 = (f32[32,8]{1,0}, f32[32,8]{1,0}, f32[128,12,12,32,1,8]{5,3,0,2,1,4}) fusion(f32[128,12,12,32,8,8]{5,3,0,2,1,4} %copy.1469), kind=kLoop, calls=%fused_computation.193\n     Allocation type: HLO temp\n     ==========================\n\n  180. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block3_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1755 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,512]{3,0,2,1} %reshape.2010, f32[512]{0} %reshape.1614, f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4451, f32[1024]{0} %fusion.3037, f32[1024]{0} %get-tuple-element.4...\n     Allocation type: HLO temp\n     ==========================\n\n  181. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block3_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1362 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[256]{0} %get-tuple-element.4081, f32[128,14,14,256]{3,0,2,1} %bitcast.48, f32[256]{0} %get-tuple-element.4374, f32[256]{0} %get-tuple-element.4375, f32[256]{0} %fusion.3780, ...\n     Allocation type: HLO temp\n     ==========================\n\n  182. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block3_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1364 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[128,12,12,256]{3,0,2,1} %reshape.1858, f32[256]{0} %reshape.1602, f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4386, f32[512]{0} %fusion.3291, f32[512]{0} %get-tuple-element....\n     Allocation type: HLO temp\n     ==========================\n\n  183. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block3_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.898 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[128,12,12,512]{3,0,2,1} %fusion.1520.remat, f32[512]{0} %get-tuple-element.4096, f32[128,12,12,512]{3,0,2,1} %get-tuple-element.3924, f32[1,1,512,256]{3,2,1,0} %get-tuple-elem...\n     Allocation type: HLO temp\n     ==========================\n\n  184. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block4_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1366 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[256]{0} %get-tuple-element.4105, f32[128,14,14,256]{3,0,2,1} %bitcast.47, f32[256]{0} %get-tuple-element.4389, f32[256]{0} %get-tuple-element.4390, f32[256]{0} %fusion.3778, ...\n     Allocation type: HLO temp\n     ==========================\n\n  185. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.567 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1775), kind=kLoop, calls=%fused_computation.546\n     Allocation type: HLO temp\n     ==========================\n\n  186. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block4_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.900 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[128,12,12,512]{3,0,2,1} %fusion.1529.remat, f32[512]{0} %get-tuple-element.4121, f32[128,12,12,512]{3,0,2,1} %fusion.2045, f32[1,1,512,512]{3,2,1,0} %get-tuple-element.4411, f...\n     Allocation type: HLO temp\n     ==========================\n\n  187. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block1_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.902 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[512]{0} %get-tuple-element.4137, f32[128,14,14,512]{3,0,2,1} %bitcast.46, f32[512]{0} %get-tuple-element.4409, f32[512]{0} %get-tuple-element.4410, f32[512]{0} %fusion.3289, f...\n     Allocation type: HLO temp\n     ==========================\n\n  188. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block1_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1747 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,512]{3,0,2,1} %reshape.2008, f32[512]{0} %reshape.1608, f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4421, f32[1024]{0} %fusion.3039, f32[1024]{0} %get-tuple-element.4...\n     Allocation type: HLO temp\n     ==========================\n\n  189. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block2_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1749 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[512]{0} %get-tuple-element.4159, f32[128,8,8,512]{3,0,2,1} %bitcast.45, f32[512]{0} %get-tuple-element.4424, f32[512]{0} %get-tuple-element.4425, f32[512]{0} %fusion.3287, f32[...\n     Allocation type: HLO temp\n     ==========================\n\n  190. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block2_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1751 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,512]{3,0,2,1} %reshape.2009, f32[512]{0} %reshape.1611, f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4436, f32[1024]{0} %fusion.3038, f32[1024]{0} %get-tuple-element.4...\n     Allocation type: HLO temp\n     ==========================\n\n  191. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block2_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1374 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.1952.remat, f32[1024]{0} %get-tuple-element.4169, f32[128,6,6,1024]{3,0,2,1} %get-tuple-element.3915, f32[1,1,1024,512]{3,2,1,0} %get-tuple-el...\n     Allocation type: HLO temp\n     ==========================\n\n  192. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block3_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1753 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[512]{0} %get-tuple-element.4175, f32[128,8,8,512]{3,0,2,1} %bitcast.44, f32[512]{0} %get-tuple-element.4439, f32[512]{0} %get-tuple-element.4440, f32[512]{0} %fusion.3285, f32[...\n     Allocation type: HLO temp\n     ==========================\n\n  193. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block2_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.896 = (f32[512]{0}, f32[512]{0}, f32[128,12,12,512]{3,0,2,1}) fusion(f32[128,12,12,512]{3,0,2,1} %fusion.1511.remat, f32[512]{0} %get-tuple-element.4076, f32[128,12,12,512]{3,0,2,1} %get-tuple-element.3927, f32[1,1,512,256]{3,2,1,0} %get-tuple-elem...\n     Allocation type: HLO temp\n     ==========================\n\n  194. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block3_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1376 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.1961.remat, f32[1024]{0} %get-tuple-element.4182, f32[128,6,6,1024]{3,0,2,1} %get-tuple-element.3912, f32[1,1,1024,512]{3,2,1,0} %get-tuple-el...\n     Allocation type: HLO temp\n     ==========================\n\n  195. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block4_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1757 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[512]{0} %get-tuple-element.4188, f32[128,8,8,512]{3,0,2,1} %bitcast.43, f32[512]{0} %get-tuple-element.4454, f32[512]{0} %get-tuple-element.4455, f32[512]{0} %fusion.3283, f32[...\n     Allocation type: HLO temp\n     ==========================\n\n  196. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block4_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1759 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,512]{3,0,2,1} %reshape.2011, f32[512]{0} %reshape.1617, f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4466, f32[1024]{0} %fusion.3036, f32[1024]{0} %get-tuple-element.4...\n     Allocation type: HLO temp\n     ==========================\n\n  197. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block4_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1378 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.1970.remat, f32[1024]{0} %get-tuple-element.4195, f32[128,6,6,1024]{3,0,2,1} %get-tuple-element.3909, f32[1,1,1024,512]{3,2,1,0} %get-tuple-el...\n     Allocation type: HLO temp\n     ==========================\n\n  198. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block5_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1761 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[512]{0} %get-tuple-element.4201, f32[128,8,8,512]{3,0,2,1} %bitcast.42, f32[512]{0} %get-tuple-element.4469, f32[512]{0} %get-tuple-element.4470, f32[512]{0} %fusion.3281, f32[...\n     Allocation type: HLO temp\n     ==========================\n\n  199. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block5_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1763 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,512]{3,0,2,1} %reshape.2012, f32[512]{0} %reshape.1620, f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4481, f32[1024]{0} %fusion.3035, f32[1024]{0} %get-tuple-element.4...\n     Allocation type: HLO temp\n     ==========================\n\n  200. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block5_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1380 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.1979.remat, f32[1024]{0} %get-tuple-element.4207, f32[128,6,6,1024]{3,0,2,1} %get-tuple-element.3906, f32[1,1,1024,512]{3,2,1,0} %get-tuple-el...\n     Allocation type: HLO temp\n     ==========================\n\n  201. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block6_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1765 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[512]{0} %get-tuple-element.4214, f32[128,8,8,512]{3,0,2,1} %bitcast.41, f32[512]{0} %get-tuple-element.4484, f32[512]{0} %get-tuple-element.4485, f32[512]{0} %fusion.3279, f32[...\n     Allocation type: HLO temp\n     ==========================\n\n  202. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block6_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1767 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,512]{3,0,2,1} %reshape.2013, f32[512]{0} %reshape.1623, f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4496, f32[1024]{0} %fusion.3034, f32[1024]{0} %get-tuple-element.4...\n     Allocation type: HLO temp\n     ==========================\n\n  203. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block6_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1382 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.1988.remat, f32[1024]{0} %get-tuple-element.4222, f32[128,6,6,1024]{3,0,2,1} %fusion.2192, f32[1,1,1024,1024]{3,2,1,0} %get-tuple-element.4506...\n     Allocation type: HLO temp\n     ==========================\n\n  204. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block1_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1384 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1024]{0} %get-tuple-element.4234, f32[128,8,8,1024]{3,0,2,1} %bitcast.40, f32[1024]{0} %get-tuple-element.4504, f32[1024]{0} %get-tuple-element.4505, f32[1024]{0} %fusion.30...\n     Allocation type: HLO temp\n     ==========================\n\n  205. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block1_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.880 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(f32[128]{0} %get-tuple-element.3950, f32[128,26,26,128]{3,0,2,1} %bitcast.53, f32[128]{0} %get-tuple-element.4294, f32[128]{0} %get-tuple-element.4295, f32[128]{0} %fusion.3988, b...\n     Allocation type: HLO temp\n     ==========================\n\n  206. Size: 12.0K\n     XLA label: profiler\n     Allocation type: reserved\n     ==========================\n\n  207. Size: 12.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv5_block1_1_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,1024,1024]{3,2,1,0}, f32[1,1,1024,1024]{3,2,1,0}, f32[1,1,1024,1024]{3,2,1,0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2134 = (f32[1,1,1024,1024]{3,2,1,0}, f32[1,1,1024,1024]{3,2,1,0}, f32[1,1,1024,1024]{3,2,1,0}) fusion(f32[1,1,1024,1024]{3,2,1,0} %get-tuple-element.4506, f32[] %divide.118, f32[1,1,1024,1024]{3,2,1,0} %get-tuple-element.4819, f32[1,1,1024,1024]{3,...\n     Allocation type: HLO temp\n     ==========================\n\n  208. Size: 12.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block6_1_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2256 = (f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}, f32[1,1,1024,512]{3,2,1,0}) fusion(f32[1,1,1024,512]{3,2,1,0} %get-tuple-element.4486, f32[] %divide.118, f32[1,1,1024,512]{3,2,1,0} %get-tuple-element.4796, f32[1,1,1024,512]{3,2,1,0}...\n     Allocation type: HLO temp\n     ==========================\n\n  209. Size: 12.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv4_block1_1_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,512,512]{3,2,1,0}, f32[1,1,512,512]{3,2,1,0}, f32[1,1,512,512]{3,2,1,0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2389 = (f32[1,1,512,512]{3,2,1,0}, f32[1,1,512,512]{3,2,1,0}, f32[1,1,512,512]{3,2,1,0}) fusion(f32[1,1,512,512]{3,2,1,0} %get-tuple-element.4411, f32[] %divide.118, f32[1,1,512,512]{3,2,1,0} %get-tuple-element.4705, f32[1,1,512,512]{3,2,1,0} %get-...\n     Allocation type: HLO temp\n     ==========================\n\n  210. Size: 12.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block1_3_bn/gamma/0/ResourceApplyAdam\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[512]{0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.4223 = (f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.4355, f32[] %divide.118, f32[512]{0} %get-tuple-element.4638, f32[512]{0} %get-tuple-element.2137, f32[512]{0} %get-tuple-element.4637), kind=kLoop, calls=%fused_c...\n     Allocation type: HLO temp\n     ==========================\n\n  211. Size: 12.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block1_1_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,256,256]{3,2,1,0}, f32[1,1,256,256]{3,2,1,0}, f32[1,1,256,256]{3,2,1,0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2538 = (f32[1,1,256,256]{3,2,1,0}, f32[1,1,256,256]{3,2,1,0}, f32[1,1,256,256]{3,2,1,0}) fusion(f32[1,1,256,256]{3,2,1,0} %get-tuple-element.4346, f32[] %divide.118, f32[1,1,256,256]{3,2,1,0} %get-tuple-element.4627, f32[1,1,256,256]{3,2,1,0} %get-...\n     Allocation type: HLO temp\n     ==========================\n\n  212. Size: 12.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv3_block1_0_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2482 = (f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}, f32[1,1,256,512]{3,2,1,0}) fusion(f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4341, f32[] %divide.118, f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4622, f32[1,1,256,512]{3,2,1,0} %get-...\n     Allocation type: HLO temp\n     ==========================\n\n  213. Size: 12.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block1_1_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,64,128]{3,2,1,0}, f32[1,1,64,128]{3,2,1,0}, f32[1,1,64,128]{3,2,1,0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.3465 = (f32[1,1,64,128]{3,2,1,0}, f32[1,1,64,128]{3,2,1,0}, f32[1,1,64,128]{3,2,1,0}) fusion(f32[1,1,64,128]{3,2,1,0} %get-tuple-element.4296, f32[] %divide.118, f32[1,1,64,128]{3,2,1,0} %get-tuple-element.4567, f32[1,1,64,128]{3,2,1,0} %get-tuple-...\n     Allocation type: HLO temp\n     ==========================\n\n  214. Size: 12.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv2_block1_0_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[1,1,64,256]{3,2,1,0}, f32[1,1,64,256]{3,2,1,0}, f32[1,1,64,256]{3,2,1,0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2978 = (f32[1,1,64,256]{3,2,1,0}, f32[1,1,64,256]{3,2,1,0}, f32[1,1,64,256]{3,2,1,0}) fusion(f32[1,1,64,256]{3,2,1,0} %get-tuple-element.4291, f32[] %divide.118, f32[1,1,64,256]{3,2,1,0} %get-tuple-element.4561, f32[1,1,64,256]{3,2,1,0} %get-tuple-...\n     Allocation type: HLO temp\n     ==========================\n\n  215. Size: 12.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//conv1_conv/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[7,7,3,64]{3,2,1,0}, f32[7,7,3,64]{3,2,1,0}, f32[7,7,3,64]{3,2,1,0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2871 = (f32[7,7,3,64]{3,2,1,0}, f32[7,7,3,64]{3,2,1,0}, f32[7,7,3,64]{3,2,1,0}) fusion(f32[7,7,3,64]{3,2,1,0} %reshape.3.remat, f32[] %divide.118, f32[7,7,3,64]{3,2,1,0} %reshape.618, f32[7,7,3,64]{3,2,1,0} %get-tuple-element.2098, f32[7,7,3,64]{3,...\n     Allocation type: HLO temp\n     ==========================\n\n  216. Size: 12.0K\n     Operator: op_type=\"ResourceApplyAdam\" op_name=\"training/TFOptimizer/Adam/update_tpu_139858372878176//3_/kernel/0/ResourceApplyAdam\"\n     Shape: (f32[22528,1]{1,0}, f32[22528,1]{1,0}, f32[22528,1]{1,0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1566 = (f32[22528,1]{1,0}, f32[22528,1]{1,0}, f32[22528,1]{1,0}) fusion(f32[] %divide.118, f32[22528]{0} %get-tuple-element.4281, f32[22528,1]{1,0} %get-tuple-element.2095, f32[22528]{0} %get-tuple-element.4549, f32[22528]{0} %get-tuple-element.455...\n     Allocation type: HLO temp\n     ==========================\n\n  217. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[64]{0}, f32[64]{0}, f32[128,48,48,64]{0,3,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.617 = (f32[64]{0}, f32[64]{0}, f32[128,48,48,64]{0,3,2,1}) fusion(f32[64]{0} %get-tuple-element.3946, f32[128,50,50,64]{0,3,2,1} %select-and-scatter, f32[64]{0} %get-tuple-element.4284, f32[64]{0} %get-tuple-element.4285, f32[64]{0} %fusion.4745, b...\n     Allocation type: HLO temp\n     ==========================\n\n  218. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block4_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1368 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[128,12,12,256]{3,0,2,1} %reshape.1859, f32[256]{0} %reshape.1605, f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4401, f32[512]{0} %fusion.3290, f32[512]{0} %get-tuple-element....\n     Allocation type: HLO temp\n     ==========================\n\n  219. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block1_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.882 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(f32[128,24,24,128]{3,0,2,1} %reshape.1746, f32[128]{0} %reshape.1587, f32[1,1,128,256]{3,2,1,0} %get-tuple-element.4306, f32[256]{0} %fusion.3787.remat, f32[256]{0} %get-tuple-ele...\n     Allocation type: HLO temp\n     ==========================\n\n  220. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block2_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.884 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(f32[128]{0} %get-tuple-element.3972, f32[128,26,26,128]{3,0,2,1} %bitcast.52, f32[128]{0} %get-tuple-element.4309, f32[128]{0} %get-tuple-element.4310, f32[128]{0} %fusion.3986.re...\n     Allocation type: HLO temp\n     ==========================\n\n  221. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block2_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.886 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(f32[128,24,24,128]{3,0,2,1} %reshape.1747, f32[128]{0} %reshape.1590, f32[1,1,128,256]{3,2,1,0} %get-tuple-element.4321, f32[256]{0} %fusion.3786.remat, f32[256]{0} %get-tuple-ele...\n     Allocation type: HLO temp\n     ==========================\n\n  222. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block2_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.623 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1} %fusion.1028.remat2, f32[256]{0} %get-tuple-element.3989, f32[128,24,24,256]{3,0,2,1} %get-tuple-element.3936, f32[1,1,256,128]{3,2,1,0} %get-tuple-ele...\n     Allocation type: HLO temp\n     ==========================\n\n  223. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block3_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.888 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(f32[128]{0} %get-tuple-element.4001, f32[128,26,26,128]{3,0,2,1} %bitcast.51, f32[128]{0} %get-tuple-element.4324, f32[128]{0} %get-tuple-element.4325, f32[128]{0} %fusion.3984.re...\n     Allocation type: HLO temp\n     ==========================\n\n  224. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block3_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.890 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(f32[128,24,24,128]{3,0,2,1} %reshape.1748, f32[128]{0} %reshape.1593, f32[1,1,128,256]{3,2,1,0} %get-tuple-element.4336, f32[256]{0} %fusion.3785, f32[256]{0} %get-tuple-element.4...\n     Allocation type: HLO temp\n     ==========================\n\n  225. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv2_block3_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.625 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1} %fusion.1036.remat, f32[256]{0} %get-tuple-element.4015, f32[128,24,24,256]{3,0,2,1} %fusion.1590, f32[1,1,256,256]{3,2,1,0} %get-tuple-element.4346, f...\n     Allocation type: HLO temp\n     ==========================\n\n  226. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block1_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.627 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[256]{0} %get-tuple-element.4033, f32[128,26,26,256]{3,0,2,1} %bitcast.50, f32[256]{0} %get-tuple-element.4344, f32[256]{0} %get-tuple-element.4345, f32[256]{0} %fusion.3784, f...\n     Allocation type: HLO temp\n     ==========================\n\n  227. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block1_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1356 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[128,12,12,256]{3,0,2,1} %reshape.1856, f32[256]{0} %reshape.1596, f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4356, f32[512]{0} %fusion.3293, f32[512]{0} %get-tuple-element....\n     Allocation type: HLO temp\n     ==========================\n\n  228. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block2_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1358 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[256]{0} %get-tuple-element.4058, f32[128,14,14,256]{3,0,2,1} %bitcast.49, f32[256]{0} %get-tuple-element.4359, f32[256]{0} %get-tuple-element.4360, f32[256]{0} %fusion.3782, ...\n     Allocation type: HLO temp\n     ==========================\n\n  229. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv3_block2_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1360 = (f32[256]{0}, f32[256]{0}, f32[128,12,12,256]{3,0,2,1}) fusion(f32[128,12,12,256]{3,0,2,1} %reshape.1857, f32[256]{0} %reshape.1599, f32[1,1,256,512]{3,2,1,0} %get-tuple-element.4371, f32[512]{0} %fusion.3292, f32[512]{0} %get-tuple-element....\n     Allocation type: HLO temp\n     ==========================\n\n  230. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.551 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1809), kind=kLoop, calls=%fused_computation.530\n     Allocation type: HLO temp\n     ==========================\n\n  231. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv5_block1_1_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1385 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.255, f32[1,1,1024,1024]{3,2,1,0} %get-tuple-element.4506), kind=kOutput, calls=%fused_computation.1273, metadata={op_type=\"FusedBatchNorm\" op_...\n     Allocation type: HLO temp\n     ==========================\n\n  232. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv5_block1_0_bn/FusedBatchNorm\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1782 = (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.255, f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4501), kind=kOutput, calls=%fused_computation.1597, metadata={op_type=\"FusedBatchNorm\" op_...\n     Allocation type: HLO temp\n     ==========================\n\n  233. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv5_block1_0_bn/FusedBatchNorm\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1718 = (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4501, f32[128,6,6,1024]{3,0,2,1} %get-tuple-element.2862, f32[1024]{0} %get-tuple-element.4494, f32[1024]{0} %get-tuple-element.44...\n     Allocation type: HLO temp\n     ==========================\n\n  234. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block6_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1326 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4496, f32[512]{0} %get-tuple-element.4489, f32[512]{0} %get-tuple-element.4490, f32[512]{0} %get-tuple-element.2944, f32[128,6,6,51...\n     Allocation type: HLO temp\n     ==========================\n\n  235. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.584 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1558), kind=kLoop, calls=%fused_computation.563\n     Allocation type: HLO temp\n     ==========================\n\n  236. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.543 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1826), kind=kLoop, calls=%fused_computation.522\n     Allocation type: HLO temp\n     ==========================\n\n  237. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block6_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1721 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %get-tuple-element.2862, f32[1,1,1024,512]{3,2,1,0} %get-tuple-element.4486), kind=kOutput, calls=%fused_computation.1546, metadata={op_type=\"FusedBatchN...\n     Allocation type: HLO temp\n     ==========================\n\n  238. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block6_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1785 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %get-tuple-element.2863, f32[1,1,1024,512]{3,2,1,0} %get-tuple-element.4486), kind=kOutput, calls=%fused_computation.1600, metadata={op_type=\"FusedBatchN...\n     Allocation type: HLO temp\n     ==========================\n\n  239. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block5_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1328 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4481, f32[512]{0} %get-tuple-element.4474, f32[512]{0} %get-tuple-element.4475, f32[512]{0} %get-tuple-element.2963, f32[128,6,6,51...\n     Allocation type: HLO temp\n     ==========================\n\n  240. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block5_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1389 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4481, f32[512]{0} %get-tuple-element.4474, f32[512]{0} %get-tuple-element.4475, f32[512]{0} %fusion.3280, f32[128,6,6,512]{3,0,2,1}...\n     Allocation type: HLO temp\n     ==========================\n\n  241. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.585 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1542), kind=kLoop, calls=%fused_computation.564\n     Allocation type: HLO temp\n     ==========================\n\n  242. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block4_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1731 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.232, f32[1,1,1024,512]{3,2,1,0} %get-tuple-element.4456), kind=kOutput, calls=%fused_computation.1552, metadata={op_type=\"FusedBatchNorm\" op_name...\n     Allocation type: HLO temp\n     ==========================\n\n  243. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block5_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1726 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[1,1,1024,512]{3,2,1,0} %get-tuple-element.4471, f32[128,6,6,1024]{3,0,2,1} %fusion.232, f32[1024]{0} %get-tuple-element.4464, f32[1024]{0} %get-tuple-element.4465, f32[1024]{0}...\n     Allocation type: HLO temp\n     ==========================\n\n  244. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block5_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1788 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.253, f32[1,1,1024,512]{3,2,1,0} %get-tuple-element.4471), kind=kOutput, calls=%fused_computation.1603, metadata={op_type=\"FusedBatchNorm\" op_name...\n     Allocation type: HLO temp\n     ==========================\n\n  245. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block4_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1330 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4466, f32[512]{0} %get-tuple-element.4459, f32[512]{0} %get-tuple-element.4460, f32[512]{0} %get-tuple-element.2985, f32[128,6,6,51...\n     Allocation type: HLO temp\n     ==========================\n\n  246. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block4_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1391 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4466, f32[512]{0} %get-tuple-element.4459, f32[512]{0} %get-tuple-element.4460, f32[512]{0} %fusion.3282, f32[128,6,6,512]{3,0,2,1}...\n     Allocation type: HLO temp\n     ==========================\n\n  247. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.586 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1526), kind=kLoop, calls=%fused_computation.565\n     Allocation type: HLO temp\n     ==========================\n\n  248. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.559 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1792), kind=kLoop, calls=%fused_computation.538\n     Allocation type: HLO temp\n     ==========================\n\n  249. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block1_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2115 = (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1}) fusion(f32[128,3,3,1024]{3,0,2,1} %reshape.2116, f32[1024]{0} %reshape.1626, f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4516, f32[2048]{0} %fusion.3483, f32[2048]{0} %get-tuple-ele...\n     Allocation type: HLO temp\n     ==========================\n\n  250. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block4_1_bn/FusedBatchNorm\"\n     Shape: (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1791 = (f32[512]{0}, f32[512]{0}, f32[128,6,6,512]{3,0,2,1}) fusion(f32[128,6,6,1024]{3,0,2,1} %fusion.252, f32[1,1,1024,512]{3,2,1,0} %get-tuple-element.4456), kind=kOutput, calls=%fused_computation.1606, metadata={op_type=\"FusedBatchNorm\" op_name...\n     Allocation type: HLO temp\n     ==========================\n\n  251. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv4_block3_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1393 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4451, f32[512]{0} %get-tuple-element.4444, f32[512]{0} %get-tuple-element.4445, f32[512]{0} %fusion.3284, f32[128,6,6,512]{3,0,2,1}...\n     Allocation type: HLO temp\n     ==========================\n\n  252. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv4_block3_3_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1332 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,512,1024]{3,2,1,0} %get-tuple-element.4451, f32[512]{0} %get-tuple-element.4444, f32[512]{0} %get-tuple-element.4445, f32[512]{0} %get-tuple-element.2994, f32[128,6,6,51...\n     Allocation type: HLO temp\n     ==========================\n\n  253. Size: 12.0K\n     Shape: (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.587 = (f32[32,16]{1,0}, f32[32,16]{1,0}, f32[128,6,6,32,1,16]{5,3,0,2,1,4}) fusion(f32[128,6,6,32,16,16]{5,3,0,2,1,4} %copy.1510), kind=kLoop, calls=%fused_computation.566\n     Allocation type: HLO temp\n     ==========================\n\n  254. Size: 12.0K\n     Shape: (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,4,3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.820 = (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,4,3,0,2,1}) fusion(f32[128,3,3,32,32,32]{5,4,3,0,2,1} %reshape.198), kind=kLoop, calls=%fused_computation.785\n     Allocation type: HLO temp\n     ==========================\n\n  255. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block2_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2117 = (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1}) fusion(f32[1024]{0} %get-tuple-element.4248, f32[128,5,5,1024]{3,0,2,1} %bitcast.39, f32[1024]{0} %get-tuple-element.4519, f32[1024]{0} %get-tuple-element.4520, f32[1024]{0} %fusion.30...\n     Allocation type: HLO temp\n     ==========================\n\n  256. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block2_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2119 = (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1}) fusion(f32[128,3,3,1024]{3,0,2,1} %reshape.2117, f32[1024]{0} %reshape.1629, f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4531, f32[2048]{0} %fusion.3482, f32[2048]{0} %get-tuple-ele...\n     Allocation type: HLO temp\n     ==========================\n\n  257. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block2_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1773 = (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[128,3,3,2048]{3,0,2,1} %fusion.2148.remat, f32[2048]{0} %get-tuple-element.4255, f32[128,3,3,2048]{3,0,2,1} %get-tuple-element.2796, f32[1,1,2048,1024]{3,2,1,0} %get-tuple-e...\n     Allocation type: HLO temp\n     ==========================\n\n  258. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block3_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.4886 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.2015, f32[1024]{0} %get-tuple-element.2089, f32[1024]{0} %get-tuple-element.2088, f32[1024]{0} %get-tuple-element.4532, f32[1024]{0} %get-tuple-element.2083), ...\n     Allocation type: HLO temp\n     ==========================\n\n  259. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block3_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2121 = (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1}) fusion(f32[1024]{0} %get-tuple-element.4262, f32[128,5,5,1024]{3,0,2,1} %bitcast.38, f32[1024]{0} %get-tuple-element.4534, f32[1024]{0} %get-tuple-element.4535, f32[1024]{0} %fusion.30...\n     Allocation type: HLO temp\n     ==========================\n\n  260. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block3_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2123 = (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1}) fusion(f32[128,3,3,1024]{3,0,2,1} %reshape.2118, f32[1024]{0} %reshape.1632, f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4546, f32[2048]{0} %get-tuple-element.2801, f32[2048]{0} %ge...\n     Allocation type: HLO temp\n     ==========================\n\n  261. Size: 12.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block3_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1775 = (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[128,3,3,2048]{3,0,2,1} %get-tuple-element.3897, f32[2048]{0} %get-tuple-element.2878, f32[128,3,3,2048]{3,0,2,1} %copy.1640, f32[128,2048]{1,0} %fusion.2327, f32[128,2048]{1...\n     Allocation type: HLO temp\n     ==========================\n\n  262. Size: 12.0K\n     Operator: op_type=\"Sum\" op_name=\"tpu_139858372878176/metrics/metrics/acc/Sum\"\n     Shape: (f32[], f32[], f32[1024,1]{1,0})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2520 = (f32[], f32[], f32[1024,1]{1,0}) fusion(f32[1024,1]{1,0} %get-tuple-element.2092, f32[1024,1]{1,0} %get-tuple-element.2093), kind=kLoop, calls=%fused_computation.2222, metadata={op_type=\"Sum\" op_name=\"tpu_139858372878176/metrics/metrics/acc/...\n     Allocation type: HLO temp\n     ==========================\n\n  263. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv5_block3_3_bn/FusedBatchNorm\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1776 = (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4546, f32[1024]{0} %get-tuple-element.4539, f32[1024]{0} %copy.2202, f32[1024]{0} %get-tuple-element.4268, f32[128,3,3,1024]{3,0,2...\n     Allocation type: HLO temp\n     ==========================\n\n  264. Size: 12.0K\n     Shape: (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1242 = (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,3,0,2,1,4}) fusion(f32[128,3,3,32,32,32]{5,3,0,2,1,4} %copy.1909), kind=kLoop, calls=%fused_computation.1146\n     Allocation type: HLO temp\n     ==========================\n\n  265. Size: 12.0K\n     Shape: (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1275 = (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,3,0,2,1,4}) fusion(f32[128,3,3,32,32,32]{5,3,0,2,1,4} %copy.1607), kind=kLoop, calls=%fused_computation.1179\n     Allocation type: HLO temp\n     ==========================\n\n  266. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv5_block3_1_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2102 = (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1}) fusion(f32[1,1,2048,1024]{3,2,1,0} %get-tuple-element.4536, f32[2048]{0} %get-tuple-element.4529, f32[2048]{0} %get-tuple-element.4530, f32[2048]{0} %get-tuple-element.4257, f32[128,3,...\n     Allocation type: HLO temp\n     ==========================\n\n  267. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv5_block2_3_bn/FusedBatchNorm\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1714 = (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4531, f32[1024]{0} %get-tuple-element.4524, f32[1024]{0} %get-tuple-element.4525, f32[1024]{0} %get-tuple-element.2893, f32[128,3,...\n     Allocation type: HLO temp\n     ==========================\n\n  268. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv5_block1_1_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1323 = (f32[1024]{0}, f32[1024]{0}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1,1,1024,1024]{3,2,1,0} %get-tuple-element.4506, f32[128,6,6,1024]{3,0,2,1} %get-tuple-element.2862, f32[1024]{0} %get-tuple-element.4494, f32[1024]{0} %get-tuple-element.44...\n     Allocation type: HLO temp\n     ==========================\n\n  269. Size: 12.0K\n     Shape: (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1274 = (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,3,0,2,1,4}) fusion(f32[128,3,3,32,32,32]{5,3,0,2,1,4} %copy.1843), kind=kLoop, calls=%fused_computation.1178\n     Allocation type: HLO temp\n     ==========================\n\n  270. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv5_block1_3_bn/FusedBatchNorm\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1717 = (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4516, f32[1024]{0} %get-tuple-element.4509, f32[1024]{0} %get-tuple-element.4510, f32[1024]{0} %get-tuple-element.2854, f32[128,3,...\n     Allocation type: HLO temp\n     ==========================\n\n  271. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv5_block1_3_bn/FusedBatchNorm\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1780 = (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4516, f32[1024]{0} %get-tuple-element.4509, f32[1024]{0} %get-tuple-element.4510, f32[1024]{0} %fusion.3032, f32[128,3,3,1024]{3,0...\n     Allocation type: HLO temp\n     ==========================\n\n  272. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv5_block2_1_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2128 = (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1}) fusion(f32[128,3,3,2048]{3,0,2,1} %fusion.718, f32[1,1,2048,1024]{3,2,1,0} %copy.2183), kind=kOutput, calls=%fused_computation.1878, metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139...\n     Allocation type: HLO temp\n     ==========================\n\n  273. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv5_block3_1_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2125 = (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1}) fusion(f32[128,3,3,2048]{3,0,2,1} %fusion.719, f32[1,1,2048,1024]{3,2,1,0} %get-tuple-element.4536), kind=kOutput, calls=%fused_computation.1875, metadata={op_type=\"FusedBatchNorm\" op_...\n     Allocation type: HLO temp\n     ==========================\n\n  274. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv5_block2_1_bn/FusedBatchNorm\"\n     Shape: (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.2108 = (f32[1024]{0}, f32[1024]{0}, f32[128,3,3,1024]{3,0,2,1}) fusion(f32[1,1,2048,1024]{3,2,1,0} %copy.2183, f32[2048]{0} %get-tuple-element.4499, f32[2048]{0} %get-tuple-element.4514, f32[2048]{0} %get-tuple-element.4515, f32[2048]{0} %get-tuple...\n     Allocation type: HLO temp\n     ==========================\n\n  275. Size: 12.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv5_block2_3_bn/FusedBatchNorm\"\n     Shape: (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1778 = (f32[2048]{0}, f32[2048]{0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4531, f32[1024]{0} %get-tuple-element.4524, f32[1024]{0} %get-tuple-element.4525, f32[1024]{0} %fusion.3030, f32[128,3,3,1024]{3,0...\n     Allocation type: HLO temp\n     ==========================\n\n  276. Size: 12.0K\n     Shape: (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1258 = (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,3,0,2,1,4}) fusion(f32[128,3,3,32,32,32]{5,3,0,2,1,4} %copy.1876), kind=kLoop, calls=%fused_computation.1162\n     Allocation type: HLO temp\n     ==========================\n\n  277. Size: 12.0K\n     Shape: (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,3,0,2,1,4})\n     Unpadded size: 12.0K\n     XLA label: %fusion.1276 = (f32[32,32]{1,0}, f32[32,32]{1,0}, f32[128,3,3,32,1,32]{5,3,0,2,1,4}) fusion(f32[128,3,3,32,32,32]{5,3,0,2,1,4} %copy.1575), kind=kLoop, calls=%fused_computation.1180\n     Allocation type: HLO temp\n     ==========================\n\n  278. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block1_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[2048]{0}, f32[2048]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4889 = (f32[2048]{0}, f32[2048]{0}) fusion(f32[2048]{0} %get-tuple-element.2019, f32[2048]{0} %get-tuple-element.2064, f32[2048]{0} %get-tuple-element.2063), kind=kLoop, calls=%fused_computation.4571, metadata={op_type=\"FusedBatchNormGrad\" op_name=...\n     Allocation type: HLO temp\n     ==========================\n\n  279. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block1_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4883 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.2084, f32[1024]{0} %reshape.1627, f32[1024]{0} %reshape.1626), kind=kLoop, calls=%fused_computation.4565, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimize...\n     Allocation type: HLO temp\n     ==========================\n\n  280. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block1_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4882 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1871, f32[1024]{0} %get-tuple-element.1983, f32[1024]{0} %get-tuple-element.1982), kind=kLoop, calls=%fused_computation.4564, metadata={op_type=\"FusedBatchNormGrad\" op_name=...\n     Allocation type: HLO temp\n     ==========================\n\n  281. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block6_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4881 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1962, f32[1024]{0} %get-tuple-element.1987, f32[1024]{0} %get-tuple-element.1986), kind=kLoop, calls=%fused_computation.4563, metadata={op_type=\"FusedBatchNormGrad\" op_name=...\n     Allocation type: HLO temp\n     ==========================\n\n  282. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block6_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4874 = (f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.2031, f32[512]{0} %reshape.1624, f32[512]{0} %reshape.1623), kind=kLoop, calls=%fused_computation.4556, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gra...\n     Allocation type: HLO temp\n     ==========================\n\n  283. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block6_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4873 = (f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.1938, f32[512]{0} %get-tuple-element.2058, f32[512]{0} %get-tuple-element.2057), kind=kLoop, calls=%fused_computation.4555, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"trai...\n     Allocation type: HLO temp\n     ==========================\n\n  284. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block5_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4880 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.2007, f32[1024]{0} %get-tuple-element.1992, f32[1024]{0} %get-tuple-element.1991), kind=kLoop, calls=%fused_computation.4562, metadata={op_type=\"FusedBatchNormGrad\" op_name=...\n     Allocation type: HLO temp\n     ==========================\n\n  285. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block5_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4872 = (f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.2065, f32[512]{0} %reshape.1621, f32[512]{0} %reshape.1620), kind=kLoop, calls=%fused_computation.4554, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gra...\n     Allocation type: HLO temp\n     ==========================\n\n  286. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block5_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4871 = (f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.1930, f32[512]{0} %get-tuple-element.2073, f32[512]{0} %get-tuple-element.2072), kind=kLoop, calls=%fused_computation.4553, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"trai...\n     Allocation type: HLO temp\n     ==========================\n\n  287. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block4_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4879 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1960, f32[1024]{0} %get-tuple-element.1979, f32[1024]{0} %get-tuple-element.1978), kind=kLoop, calls=%fused_computation.4561, metadata={op_type=\"FusedBatchNormGrad\" op_name=...\n     Allocation type: HLO temp\n     ==========================\n\n  288. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block4_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4870 = (f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.2021, f32[512]{0} %reshape.1618, f32[512]{0} %reshape.1617), kind=kLoop, calls=%fused_computation.4552, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gra...\n     Allocation type: HLO temp\n     ==========================\n\n  289. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block4_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4869 = (f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.1936, f32[512]{0} %get-tuple-element.2034, f32[512]{0} %get-tuple-element.2033), kind=kLoop, calls=%fused_computation.4551, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"trai...\n     Allocation type: HLO temp\n     ==========================\n\n  290. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block3_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4878 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1999, f32[1024]{0} %get-tuple-element.1947, f32[1024]{0} %get-tuple-element.1946), kind=kLoop, calls=%fused_computation.4560, metadata={op_type=\"FusedBatchNormGrad\" op_name=...\n     Allocation type: HLO temp\n     ==========================\n\n  291. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block3_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4868 = (f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.2067, f32[512]{0} %reshape.1615, f32[512]{0} %reshape.1614), kind=kLoop, calls=%fused_computation.4550, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gra...\n     Allocation type: HLO temp\n     ==========================\n\n  292. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block3_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4867 = (f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.1934, f32[512]{0} %get-tuple-element.2024, f32[512]{0} %get-tuple-element.2023), kind=kLoop, calls=%fused_computation.4549, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"trai...\n     Allocation type: HLO temp\n     ==========================\n\n  293. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block2_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4877 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.1993, f32[1024]{0} %get-tuple-element.1943, f32[1024]{0} %get-tuple-element.1942), kind=kLoop, calls=%fused_computation.4559, metadata={op_type=\"FusedBatchNormGrad\" op_name=...\n     Allocation type: HLO temp\n     ==========================\n\n  294. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv4_block2_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[512]{0}, f32[512]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4866 = (f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %get-tuple-element.2035, f32[512]{0} %reshape.1612, f32[512]{0} %reshape.1611), kind=kLoop, calls=%fused_computation.4548, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gra...\n     Allocation type: HLO temp\n     ==========================\n\n  295. Size: 8.0K\n     Shape: s32[2048]{0}\n     Unpadded size: 8.0K\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  296. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block2_1_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4884 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.2017, f32[1024]{0} %get-tuple-element.2087, f32[1024]{0} %get-tuple-element.2086), kind=kLoop, calls=%fused_computation.4566, metadata={op_type=\"FusedBatchNormGrad\" op_name=...\n     Allocation type: HLO temp\n     ==========================\n\n  297. Size: 8.0K\n     Shape: ((s32[128]{0}, f32[3538944]{0}, f32[128]{0}), token[])\n     Unpadded size: 8.0K\n     XLA label: %infeed.1 = ((s32[128]{0}, f32[3538944]{0}, f32[128]{0}), token[]) infeed(token[] %after-all)\n     Allocation type: HLO temp\n     ==========================\n\n  298. Size: 8.0K\n     Shape: (f32[64]{0}, f32[64]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.5278 = (f32[64]{0}, f32[64]{0}) fusion(f32[64]{0} %get-tuple-element.1831, f32[64]{0} %get-tuple-element.1830), kind=kLoop, calls=%fused_computation.4185.clone.3.clone\n     Allocation type: HLO temp\n     ==========================\n\n  299. Size: 8.0K\n     Shape: (f32[128]{0}, f32[128]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.5279 = (f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.1923, f32[128]{0} %get-tuple-element.1922), kind=kLoop, calls=%fused_computation.3691.clone.4\n     Allocation type: HLO temp\n     ==========================\n\n  300. Size: 8.0K\n     Shape: (f32[256]{0}, f32[256]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.5282 = (f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1850, f32[256]{0} %get-tuple-element.1849), kind=kLoop, calls=%fused_computation.3533.clone.4\n     Allocation type: HLO temp\n     ==========================\n\n  301. Size: 8.0K\n     Operator: op_type=\"Relu\" op_name=\"tpu_139858372878176/conv2_block1_out/Relu\"\n     Shape: (f32[128,24,24,256]{3,0,2,1}, f32[128,24,24,256]{3,0,2,1})\n     Unpadded size: 8.0K\n     XLA label: %fusion.94 = (f32[128,24,24,256]{3,0,2,1}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[256]{0} %get-tuple-element.4304, f32[256]{0} %get-tuple-element.4305, f32[256]{0} %get-tuple-element.3965, f32[128,24,24,256]{3,0,2,1} %get-tuple-element.3831, f32[256]{0} %...\n     Allocation type: HLO temp\n     ==========================\n\n  302. Size: 8.0K\n     Operator: op_type=\"Relu\" op_name=\"tpu_139858372878176/conv4_block5_out/Relu\"\n     Shape: (f32[128,6,6,1024]{3,0,2,1}, f32[128,6,6,1024]{3,0,2,1})\n     Unpadded size: 8.0K\n     XLA label: %fusion.230 = (f32[128,6,6,1024]{3,0,2,1}, f32[128,6,6,1024]{3,0,2,1}) fusion(f32[1024]{0} %get-tuple-element.4479, f32[1024]{0} %get-tuple-element.4480, f32[1024]{0} %get-tuple-element.4212, f32[128,6,6,1024]{3,0,2,1} %get-tuple-element.3880, f32[1024]{0}...\n     Allocation type: HLO temp\n     ==========================\n\n  303. Size: 8.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv5_block3_3_bn/FusedBatchNorm\"\n     Shape: (f32[2048]{0}, f32[2048]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.1713 = (f32[2048]{0}, f32[2048]{0}) fusion(f32[1,1,1024,2048]{3,2,1,0} %get-tuple-element.4546, f32[1024]{0} %get-tuple-element.4539, f32[1024]{0} %copy.2202, f32[1024]{0} %get-tuple-element.4273, f32[128,3,3,1024]{3,0,2,1} %reshape.2121, f32[1024]...\n     Allocation type: HLO temp\n     ==========================\n\n  304. Size: 8.0K\n     Operator: op_type=\"Max\" op_name=\"tpu_139858372878176/global_max_pooling2d/Max\"\n     Shape: (f32[128,2048]{1,0}, f32[128,3,3,2048]{3,0,2,1})\n     Unpadded size: 8.0K\n     XLA label: %fusion.5258 = (f32[128,2048]{1,0}, f32[128,3,3,2048]{3,0,2,1}) fusion(f32[128,3,3,2048]{3,0,2,1} %fusion.719, f32[2048]{0} %get-tuple-element.4544, f32[2048]{0} %get-tuple-element.4545, f32[2048]{0} %get-tuple-element.2801, f32[128,3,3,2048]{3,0,2,1} %get...\n     Allocation type: HLO temp\n     ==========================\n\n  305. Size: 8.0K\n     Operator: op_type=\"Sum\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/global_max_pooling2d/Max_grad/Sum\"\n     Shape: (f32[128,2048]{1,0}, f32[128,2048]{1,0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.1708 = (f32[128,2048]{1,0}, f32[128,2048]{1,0}) fusion(f32[128,3,3,2048]{3,0,2,1} %get-tuple-element.2800, f32[128,2048]{1,0} %get-tuple-element.2799), kind=kLoop, calls=%fused_computation.1535, metadata={op_type=\"Sum\" op_name=\"training/TFOptimizer...\n     Allocation type: HLO temp\n     ==========================\n\n  306. Size: 8.0K\n     Operator: op_type=\"Mul\" op_name=\"tpu_139858372878176/mul\"\n     Shape: (f32[8,128,1]{2,1,0}, f32[8,128,1]{2,1,0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.2511 = (f32[8,128,1]{2,1,0}, f32[8,128,1]{2,1,0}) fusion(f32[8]{0} %fusion.4748, f32[128,1]{1,0} %fusion.2794, f32[128]{0} %get-tuple-element.1780), kind=kLoop, calls=%fused_computation.2213, metadata={op_type=\"Mul\" op_name=\"tpu_139858372878176/mul...\n     Allocation type: HLO temp\n     ==========================\n\n  307. Size: 8.0K\n     Shape: (f32[1024,1]{1,0}, f32[1024,1]{1,0})\n     Unpadded size: 8.0K\n     XLA label: %cross-replica-sum.164 = (f32[1024,1]{1,0}, f32[1024,1]{1,0}) cross-replica-sum(f32[1024,1]{1,0} %bitcast.2, f32[1024,1]{1,0} %bitcast.1), replica_groups={{0,1,2,3,4,5,6,7}}, barrier=\"custom:0\", to_apply=%sum.619\n     Allocation type: HLO temp\n     ==========================\n\n  308. Size: 8.0K\n     Operator: op_type=\"OutfeedEnqueueTuple\" op_name=\"outfeed-enqueue-train\"\n     Shape: (f32[], f32[])\n     Unpadded size: 8.0K\n     XLA label: %tuple.53 = (f32[], f32[]) tuple(f32[] %multiply.2307, f32[] %select.2), metadata={op_type=\"OutfeedEnqueueTuple\" op_name=\"outfeed-enqueue-train\"}\n     Allocation type: HLO temp\n     ==========================\n\n  309. Size: 8.0K\n     Shape: (f32[], f32[128]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.5256 = (f32[], f32[128]{0}) fusion(f32[128]{0} %reduce.383, f32[128]{0} %fusion.1559, f32[] %bitcast.3), kind=kLoop, calls=%fused_computation.4578\n     Allocation type: HLO temp\n     ==========================\n\n  310. Size: 8.0K\n     Operator: op_type=\"RealDiv\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/dropout/dropout/truediv_grad/RealDiv\"\n     Shape: (f32[128,22528]{1,0}, f32[22528]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.1066 = (f32[128,22528]{1,0}, f32[22528]{0}) fusion(f32[22528]{0} %get-tuple-element.4281, f32[128]{0} %get-tuple-element.2798, f32[128,22528]{1,0} %rng, f32[128,18432]{1,0} %reshape.304, f32[128,2048]{1,0} %fusion.2328, f32[128,2048]{1,0} %get-tupl...\n     Allocation type: HLO temp\n     ==========================\n\n  311. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block2_3_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[2048]{0}, f32[2048]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4890 = (f32[2048]{0}, f32[2048]{0}) fusion(f32[2048]{0} %get-tuple-element.2049, f32[2048]{0} %get-tuple-element.2042, f32[2048]{0} %get-tuple-element.2041), kind=kLoop, calls=%fused_computation.4572, metadata={op_type=\"FusedBatchNormGrad\" op_name=...\n     Allocation type: HLO temp\n     ==========================\n\n  312. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block2_2_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[1024]{0}, f32[1024]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4885 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %get-tuple-element.2080, f32[1024]{0} %reshape.1630, f32[1024]{0} %reshape.1629), kind=kLoop, calls=%fused_computation.4567, metadata={op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimize...\n     Allocation type: HLO temp\n     ==========================\n\n  313. Size: 8.0K\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/TFOptimizer/gradients/tpu_139858372878176/resnext50/conv5_block1_0_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: (f32[2048]{0}, f32[2048]{0})\n     Unpadded size: 8.0K\n     XLA label: %fusion.4888 = (f32[2048]{0}, f32[2048]{0}) fusion(f32[2048]{0} %get-tuple-element.2071, f32[2048]{0} %get-tuple-element.2038, f32[2048]{0} %get-tuple-element.2037), kind=kLoop, calls=%fused_computation.4570, metadata={op_type=\"FusedBatchNormGrad\" op_name=...\n     Allocation type: HLO temp\n     ==========================\n\n  314. Size: 4.0K\n     Shape: (bf16[128,24,24,32,4,4]{5,3,0,2,1,4})\n     Unpadded size: 4.0K\n     XLA label: %fusion.5323 = (bf16[128,24,24,32,4,4]{5,3,0,2,1,4}) fusion(f32[128,24,24,32,1,4]{5,3,0,2,1,4} %reshape.1414), kind=kLoop, calls=%fused_computation.11.clone.1\n     Allocation type: HLO temp\n     ==========================\n\n  315. Size: 4.0K\n     Shape: (bf16[128,24,24,32,4,4]{5,3,0,2,1,4})\n     Unpadded size: 4.0K\n     XLA label: %fusion.5324 = (bf16[128,24,24,32,4,4]{5,3,0,2,1,4}) fusion(f32[128,24,24,32,1,4]{5,3,0,2,1,4} %reshape.1419), kind=kLoop, calls=%fused_computation.10.clone.1\n     Allocation type: HLO temp\n     ==========================\n\n  316. Size: 4.0K\n     Shape: (bf16[128,24,24,32,4,4]{5,3,0,2,1,4})\n     Unpadded size: 4.0K\n     XLA label: %fusion.5325 = (bf16[128,24,24,32,4,4]{5,3,0,2,1,4}) fusion(f32[128,24,24,32,1,4]{5,3,0,2,1,4} %reshape.1424), kind=kLoop, calls=%fused_computation.9.clone.1\n     Allocation type: HLO temp\n     ==========================\n\n  317. Size: 2.0K\n     Shape: s32[512]{0}\n     Unpadded size: 2.0K\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  318. Size: 1.0K\n     Operator: op_type=\"OutfeedEnqueueTuple\" op_name=\"outfeed-enqueue-train\"\n     Shape: f32[]\n     Unpadded size: 4B\n     Extra memory due to padding: 1020B (256.0x expansion)\n     XLA label: %outfeed = token[] outfeed((f32[], f32[]), token[])\n     Allocation type: scoped\n     ==========================\n\n  319. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_0_bn/FusedBatchNorm\"\n     Shape: f32[256]{0}\n     Unpadded size: 1.0K\n     XLA label: %fusion.636 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(bf16[128,24,24,64]{0,3,2,1} %reduce-window.remat, f32[1,1,64,256]{3,2,1,0} %get-tuple-element.4291), kind=kOutput, calls=%fused_computation.611, metadata={op_type=\"FusedBatchNorm\"...\n     Allocation type: HLO temp\n     ==========================\n\n  320. Size: 1.0K\n     Shape: s32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %infeed.1 = ((s32[128]{0}, f32[3538944]{0}, f32[128]{0}), token[]) infeed(token[] %after-all)\n     Allocation type: HLO temp\n     ==========================\n\n  321. Size: 1.0K\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %infeed.1 = ((s32[128]{0}, f32[3538944]{0}, f32[128]{0}), token[]) infeed(token[] %after-all)\n     Allocation type: HLO temp\n     ==========================\n\n  322. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv1_bn/FusedBatchNorm\"\n     Shape: f32[64]{0}\n     Unpadded size: 256B\n     Extra memory due to padding: 768B (4.0x expansion)\n     XLA label: %fusion.638 = (f32[64]{0}, f32[64]{0}, f32[128,48,48,64]{0,3,2,1}) fusion(bf16[128,96,96,3]{0,3,2,1} %copy.1455, bf16[7,7,3,64]{3,2,1,0} %reshape.3), kind=kOutput, calls=%fused_computation.613, metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_13985837287817...\n     Allocation type: HLO temp\n     ==========================\n\n  323. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv1_bn/FusedBatchNorm\"\n     Shape: f32[64]{0}\n     Unpadded size: 256B\n     Extra memory due to padding: 768B (4.0x expansion)\n     XLA label: %fusion.638 = (f32[64]{0}, f32[64]{0}, f32[128,48,48,64]{0,3,2,1}) fusion(bf16[128,96,96,3]{0,3,2,1} %copy.1455, bf16[7,7,3,64]{3,2,1,0} %reshape.3), kind=kOutput, calls=%fused_computation.613, metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_13985837287817...\n     Allocation type: HLO temp\n     ==========================\n\n  324. Size: 1.0K\n     Shape: f32[64]{0}\n     Unpadded size: 256B\n     Extra memory due to padding: 768B (4.0x expansion)\n     XLA label: %fusion.5278 = (f32[64]{0}, f32[64]{0}) fusion(f32[64]{0} %get-tuple-element.1831, f32[64]{0} %get-tuple-element.1830), kind=kLoop, calls=%fused_computation.4185.clone.3.clone\n     Allocation type: HLO temp\n     ==========================\n\n  325. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv1_bn/FusedBatchNorm\"\n     Shape: f32[64]{0}\n     Unpadded size: 256B\n     Extra memory due to padding: 768B (4.0x expansion)\n     XLA label: %fusion.4745 = f32[64]{0} fusion(f32[64]{0} %get-tuple-element.3948), kind=kLoop, calls=%fused_computation.4427, metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv1_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n  326. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_1_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %fusion.922 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(bf16[128,24,24,64]{0,3,2,1} %reduce-window, f32[1,1,64,128]{3,2,1,0} %get-tuple-element.4296), kind=kOutput, calls=%fused_computation.875, metadata={op_type=\"FusedBatchNorm\" op_na...\n     Allocation type: HLO temp\n     ==========================\n\n  327. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_1_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %fusion.922 = (f32[128]{0}, f32[128]{0}, f32[128,24,24,128]{3,0,2,1}) fusion(bf16[128,24,24,64]{0,3,2,1} %reduce-window, f32[1,1,64,128]{3,2,1,0} %get-tuple-element.4296), kind=kOutput, calls=%fused_computation.875, metadata={op_type=\"FusedBatchNorm\" op_na...\n     Allocation type: HLO temp\n     ==========================\n\n  328. Size: 1.0K\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %fusion.5279 = (f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.1923, f32[128]{0} %get-tuple-element.1922), kind=kLoop, calls=%fused_computation.3691.clone.4\n     Allocation type: HLO temp\n     ==========================\n\n  329. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_1_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %fusion.3988 = f32[128]{0} fusion(f32[128]{0} %get-tuple-element.3951), kind=kLoop, calls=%fused_computation.3670, metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_1_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n  330. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_2_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %reshape.1588 = f32[128]{0} reshape(f32[32,4]{1,0} %fusion.2869), metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_2_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n  331. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_2_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %reshape.1587 = f32[128]{0} reshape(f32[32,4]{1,0} %fusion.2870), metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_2_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n  332. Size: 1.0K\n     Shape: f32[256]{0}\n     Unpadded size: 1.0K\n     XLA label: %fusion.5281 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1846, f32[256]{0} %get-tuple-element.1845, f32[256]{0} %get-tuple-element.4302, f32[256]{0} %get-tuple-element.1833, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  333. Size: 1.0K\n     Shape: f32[256]{0}\n     Unpadded size: 1.0K\n     XLA label: %fusion.5281 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1846, f32[256]{0} %get-tuple-element.1845, f32[256]{0} %get-tuple-element.4302, f32[256]{0} %get-tuple-element.1833, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  334. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_0_bn/FusedBatchNorm\"\n     Shape: f32[256]{0}\n     Unpadded size: 1.0K\n     XLA label: %fusion.636 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(bf16[128,24,24,64]{0,3,2,1} %reduce-window.remat, f32[1,1,64,256]{3,2,1,0} %get-tuple-element.4291), kind=kOutput, calls=%fused_computation.611, metadata={op_type=\"FusedBatchNorm\"...\n     Allocation type: HLO temp\n     ==========================\n\n  335. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block3_2_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %reshape.1642 = f32[128]{0} reshape(f32[32,4]{1,0} %fusion.2859), metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block3_2_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n  336. Size: 1.0K\n     Shape: f32[256]{0}\n     Unpadded size: 1.0K\n     XLA label: %fusion.5282 = (f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1850, f32[256]{0} %get-tuple-element.1849), kind=kLoop, calls=%fused_computation.3533.clone.4\n     Allocation type: HLO temp\n     ==========================\n\n  337. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_0_bn/FusedBatchNorm\"\n     Shape: f32[256]{0}\n     Unpadded size: 1.0K\n     XLA label: %fusion.3788 = f32[256]{0} fusion(f32[256]{0} %get-tuple-element.3971), kind=kLoop, calls=%fused_computation.3470, metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block1_0_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n  338. Size: 1.0K\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %fusion.5283 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.1891, f32[128]{0} %get-tuple-element.1892, f32[128]{0} %get-tuple-element.4307, f32[128]{0} %get-tuple-element.1890, f32[128...\n     Allocation type: HLO temp\n     ==========================\n\n  339. Size: 1.0K\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %fusion.5283 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.1891, f32[128]{0} %get-tuple-element.1892, f32[128]{0} %get-tuple-element.4307, f32[128]{0} %get-tuple-element.1890, f32[128...\n     Allocation type: HLO temp\n     ==========================\n\n  340. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block2_2_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %reshape.1591 = f32[128]{0} reshape(f32[32,4]{1,0} %fusion.2867), metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block2_2_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n  341. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block2_2_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %reshape.1590 = f32[128]{0} reshape(f32[32,4]{1,0} %fusion.2868), metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/resnext50/conv2_block2_2_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n  342. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block2_2_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %reshape.1638 = f32[128]{0} reshape(f32[32,4]{1,0} %fusion.2862), metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block2_2_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n  343. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block2_2_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %reshape.1639 = f32[128]{0} reshape(f32[32,4]{1,0} %fusion.2861), metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block2_2_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n  344. Size: 1.0K\n     Shape: f32[256]{0}\n     Unpadded size: 1.0K\n     XLA label: %fusion.5285 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1867, f32[256]{0} %get-tuple-element.1868, f32[256]{0} %get-tuple-element.1827, f32[256]{0} %get-tuple-element.4317, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  345. Size: 1.0K\n     Shape: f32[256]{0}\n     Unpadded size: 1.0K\n     XLA label: %fusion.5285 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1867, f32[256]{0} %get-tuple-element.1868, f32[256]{0} %get-tuple-element.1827, f32[256]{0} %get-tuple-element.4317, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  346. Size: 1.0K\n     Shape: f32[256]{0}\n     Unpadded size: 1.0K\n     XLA label: %fusion.5285 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1867, f32[256]{0} %get-tuple-element.1868, f32[256]{0} %get-tuple-element.1827, f32[256]{0} %get-tuple-element.4317, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  347. Size: 1.0K\n     Shape: f32[256]{0}\n     Unpadded size: 1.0K\n     XLA label: %fusion.5285 = (f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}, f32[256]{0}) fusion(f32[256]{0} %get-tuple-element.1867, f32[256]{0} %get-tuple-element.1868, f32[256]{0} %get-tuple-element.1827, f32[256]{0} %get-tuple-element.4317, f32[256...\n     Allocation type: HLO temp\n     ==========================\n\n  348. Size: 1.0K\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %fusion.5286 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.1918, f32[128]{0} %get-tuple-element.1919, f32[128]{0} %get-tuple-element.4322, f32[128]{0} %get-tuple-element.1888, f32[128]{0} %get-tup...\n     Allocation type: HLO temp\n     ==========================\n\n  349. Size: 1.0K\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %fusion.5286 = (f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}, f32[128]{0}) fusion(f32[128]{0} %get-tuple-element.1918, f32[128]{0} %get-tuple-element.1919, f32[128]{0} %get-tuple-element.4322, f32[128]{0} %get-tuple-element.1888, f32[128]{0} %get-tup...\n     Allocation type: HLO temp\n     ==========================\n\n  350. Size: 1.0K\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block3_2_bn/FusedBatchNorm\"\n     Shape: f32[128]{0}\n     Unpadded size: 512B\n     Extra memory due to padding: 512B (2.0x expansion)\n     XLA label: %reshape.1641 = f32[128]{0} reshape(f32[32,4]{1,0} %fusion.2860), metadata={op_type=\"FusedBatchNorm\" op_name=\"tpu_139858372878176/conv2_block3_2_bn/FusedBatchNorm\"}\n     Allocation type: HLO temp\n     ==========================\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi56qk0Q1BXi",
        "colab_type": "code",
        "outputId": "8c401c6c-28da-4287-9205-6ef73ebe7c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "ensemble_preds /= n_fold\n",
        "\n",
        "df = pd.DataFrame({'id': ids, 'label': ensemble_preds.ravel()})\n",
        "df.to_csv(\"ensemble.csv\", index=False)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f1734ed91a06221dfbaba86f3d5448457c4679bf</td>\n",
              "      <td>0.002079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a43da378e3caf45599866986f5df757b11cc77b0</td>\n",
              "      <td>0.483096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a3af88e93ec37ab83cc8aece91faf76103201a7d</td>\n",
              "      <td>0.001331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4f67ca54a271d1ae865b9c1a6fac44de321b35dc</td>\n",
              "      <td>0.000094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ab50d6941479436cce6c022ff54e089911a812d8</td>\n",
              "      <td>0.457424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id     label\n",
              "0  f1734ed91a06221dfbaba86f3d5448457c4679bf  0.002079\n",
              "1  a43da378e3caf45599866986f5df757b11cc77b0  0.483096\n",
              "2  a3af88e93ec37ab83cc8aece91faf76103201a7d  0.001331\n",
              "3  4f67ca54a271d1ae865b9c1a6fac44de321b35dc  0.000094\n",
              "4  ab50d6941479436cce6c022ff54e089911a812d8  0.457424"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}